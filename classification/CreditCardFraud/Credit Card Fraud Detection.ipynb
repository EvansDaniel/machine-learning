{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Read-data-and-minor-inspection\" data-toc-modified-id=\"Read-data-and-minor-inspection-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Read data and minor inspection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dr.-Rudd's-question:-Is-there-a-correlation-between-higher-transaction-amounts-and-fraudulent-transactions?-I\" data-toc-modified-id=\"Dr.-Rudd's-question:-Is-there-a-correlation-between-higher-transaction-amounts-and-fraudulent-transactions?-I-0.1.1\"><span class=\"toc-item-num\">0.1.1&nbsp;&nbsp;</span>Dr. Rudd's question: Is there a correlation between higher transaction amounts and fraudulent transactions? I</a></span><ul class=\"toc-item\"><li><span><a href=\"#Source:-https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now\" data-toc-modified-id=\"Source:-https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now-0.1.1.1\"><span class=\"toc-item-num\">0.1.1.1&nbsp;&nbsp;</span>Source: <a href=\"https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now\" target=\"_blank\">https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now</a></a></span></li><li><span><a href=\"#Goal-of-this-notebook-is-to-recreate-the-success-of-the-above-source-notebook-via-the-given-resampling-method,-understand-why-it-works,-and-to-work-through-the-code-(specifically-learn-more-about-cross-validation/model-evaluation-techniques)\" data-toc-modified-id=\"Goal-of-this-notebook-is-to-recreate-the-success-of-the-above-source-notebook-via-the-given-resampling-method,-understand-why-it-works,-and-to-work-through-the-code-(specifically-learn-more-about-cross-validation/model-evaluation-techniques)-0.1.1.2\"><span class=\"toc-item-num\">0.1.1.2&nbsp;&nbsp;</span>Goal of this notebook is to recreate the success of the above source notebook via the given resampling method, understand why it works, and to work through the code (specifically learn more about cross validation/model evaluation techniques)</a></span></li><li><span><a href=\"#As-we-know,-due-to-the-imbalacing-of-the-data,-many-observations-could-be-predicted-as-False-Negatives-e.g.-fraudulent-transactions-incorrectly-classified\" data-toc-modified-id=\"As-we-know,-due-to-the-imbalacing-of-the-data,-many-observations-could-be-predicted-as-False-Negatives-e.g.-fraudulent-transactions-incorrectly-classified-0.1.1.3\"><span class=\"toc-item-num\">0.1.1.3&nbsp;&nbsp;</span>As we know, due to the imbalacing of the data, many observations could be predicted as False Negatives e.g. fraudulent transactions incorrectly classified</a></span></li><li><span><a href=\"#Having-tested-our-previous-approach,-I-find-really-interesting-to-test-the-same-process-on-the-skewed-data.-Our-intuition-is-that-skewness-will-introduce-issues-difficult-to-capture,-and-therefore,-provide-a-less-effective-algorithm.\" data-toc-modified-id=\"Having-tested-our-previous-approach,-I-find-really-interesting-to-test-the-same-process-on-the-skewed-data.-Our-intuition-is-that-skewness-will-introduce-issues-difficult-to-capture,-and-therefore,-provide-a-less-effective-algorithm.-0.1.1.4\"><span class=\"toc-item-num\">0.1.1.4&nbsp;&nbsp;</span>Having tested our previous approach, I find really interesting to test the same process on the skewed data. Our intuition is that skewness will introduce issues difficult to capture, and therefore, provide a less effective algorithm.</a></span></li></ul></li><li><span><a href=\"#Before-continuing...-changing-classification-threshold.\" data-toc-modified-id=\"Before-continuing...-changing-classification-threshold.-0.1.2\"><span class=\"toc-item-num\">0.1.2&nbsp;&nbsp;</span>Before continuing... changing classification threshold.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Let's-check-this-using-the-undersampled-data-(best-C_param-=-0.01)\" data-toc-modified-id=\"Let's-check-this-using-the-undersampled-data-(best-C_param-=-0.01)-0.1.2.1\"><span class=\"toc-item-num\">0.1.2.1&nbsp;&nbsp;</span>Let's check this using the undersampled data (best C_param = 0.01)</a></span></li><li><span><a href=\"#Let's-implement-Brian's-idea-and-see-if-we-can-improve-the-accuracy-with-a-high-degree-of-recall\" data-toc-modified-id=\"Let's-implement-Brian's-idea-and-see-if-we-can-improve-the-accuracy-with-a-high-degree-of-recall-0.1.2.2\"><span class=\"toc-item-num\">0.1.2.2&nbsp;&nbsp;</span>Let's implement Brian's idea and see if we can improve the accuracy with a high degree of recall</a></span></li><li><span><a href=\"#Based-on-the-modified-&quot;printing_Kfold_scores&quot;-function,-the-origininal-best-C-parameter-is-very-bad-for-precision.-So-assuming-it-is-less-costly-to-have-7%-less-recall-(meaning-we-catch-slightly-less-fradulent-payments)-and-28%-more-accuracy/precision-(meaning-we-have-a-lot-less-false-positives)-,-then-then-the-best-C-parameter-to-choose-would-be-0.1\" data-toc-modified-id=\"Based-on-the-modified-&quot;printing_Kfold_scores&quot;-function,-the-origininal-best-C-parameter-is-very-bad-for-precision.-So-assuming-it-is-less-costly-to-have-7%-less-recall-(meaning-we-catch-slightly-less-fradulent-payments)-and-28%-more-accuracy/precision-(meaning-we-have-a-lot-less-false-positives)-,-then-then-the-best-C-parameter-to-choose-would-be-0.1-0.1.2.3\"><span class=\"toc-item-num\">0.1.2.3&nbsp;&nbsp;</span>Based on the modified \"printing_Kfold_scores\" function, the origininal best C parameter is very bad for precision. So assuming it is less costly to have 7% less recall (meaning we catch slightly less fradulent payments) and 28% more accuracy/precision (meaning we have a lot less false positives) , then then the best C parameter to choose would be 0.1</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../shared\"))\n",
    "import eda\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and minor inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAETCAYAAADge6tNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDBtJUneGjSSpO8NGktTd/wfPdsKHs6aZdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c5cb49630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(data['Class'], sort = True).sort_index();\n",
    "count_classes.plot(kind = 'bar');\n",
    "plt.title(\"Fraud class histogram\");\n",
    "plt.xlabel(\"Class\");\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dr. Rudd's question: Is there a correlation between higher transaction amounts and fraudulent transactions? I\n",
    "It appears that there is some correlation between the variables Amount and Class, but maybe not much. It is hard to tell with the skewed response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     492.000000\n",
       "mean      122.211321\n",
       "std       256.683288\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         9.250000\n",
       "75%       105.890000\n",
       "max      2125.870000\n",
       "Name: Amount, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[data['Class']==1]['Amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    284315.000000\n",
       "mean         88.291022\n",
       "std         250.105092\n",
       "min           0.000000\n",
       "25%           5.650000\n",
       "50%          22.000000\n",
       "75%          77.050000\n",
       "max       25691.160000\n",
       "Name: Amount, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[data['Class']==0]['Amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEYCAYAAAAXsVIGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c722c5cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fraud = data1[data['Class']==1]\n",
    "non_fraud = data1[data['Class']==0]\n",
    "f, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.hist(fraud['Amount'], bins=50)\n",
    "ax1.set_title('Fraudulent Transactions')\n",
    "ax2.hist(non_fraud['Amount'], bins=50)\n",
    "ax2.set_title('Non-fraudulent Transactions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XuM3Od93/v393eZy+6SXFKkJUVU\nIjuV06PmNKqzcISmLZwEtWWf9ijp6QmcorGQ+lQ9qN3TFEVhpyjgIOkB0j96sXNxqzSK7aK1YtRN\nrLZOFcF1mvNH7IhKXNvyJWZkXUhR5Iq73Mtcftfv+eP57XJILbm71CyHu/N5AYOdeeY3M88Sy/3s\nczd3R0REZByiSVdAREQODoWKiIiMjUJFRETGRqEiIiJjo1AREZGxUaiIiMjYKFRERGRsFCoiIjI2\nChURERmbZNIVuNmOHz/u99xzz6SrISKyrzzzzDOvuvuJ7a6bulC55557OHXq1KSrISKyr5jZCzu5\nTt1fIiIyNgoVEREZG4WKiIiMjUJFRETGRqEiIiJjo1CRPVFUNWVVT7oaInKTTd2UYtlb7s5yv6Bo\nAqWdRMzPtCZcKxG5WdRSkbEaFNVmoABkZc2wqCZYIxG5mRQqMla1b1W2RaGIHEgKFRmrdnLlj5QB\n7SSeTGVE5Kbbs1Axs7vN7PNm9jUze9bM/n5T/rNmdtbMvtTc3jXymp8xs9Nm9k0ze8dI+YNN2Wkz\n++BI+RvN7ItN+W+YmTrvJyyNI+ZnUtpJRCeJOTrbIo5s0tUSkZtkL1sqJfAP3f0+4AHgfWZ2X/Pc\nv3T3+5vbZwGa594N/BngQeBXzCw2sxj4ZeCdwH3AT4y8zz9r3utPAcvAe/fw+5Edaicx8zMtjsyk\npLEawyLTZM/+x7v7OXf/w+b+GvB14K7rvOQh4HF3z9z928Bp4K3N7bS7P+fuOfA48JCZGfDDwH9s\nXv9x4Ef35rsREZGduCl/RprZPcCfA77YFL3fzL5sZo+Z2dGm7C7gpZGXnWnKrlV+G3DJ3curyrf6\n/EfM7JSZnVpcXBzDdyQiIlvZ81Axszng08BPu/sq8FHgu4H7gXPAP9/rOrj7o+6+4O4LJ05sexyA\niIjcoD1d/GhmKSFQ/r27/ycAdz8/8vyvAv+leXgWuHvk5SebMq5RfhGYN7Okaa2MXi8iIhOwl7O/\nDPg14Ovu/i9Gyu8cuezHgK82958A3m1mbTN7I3Av8AfA08C9zUyvFmEw/wl3d+DzwF9vXv8w8Jm9\n+n5ERGR7e9lS+UHgJ4GvmNmXmrJ/TJi9dT/gwPPA3wFw92fN7FPA1wgzx97n7hWAmb0feBKIgcfc\n/dnm/T4APG5m/xT4I0KIiYjIhJhP2WrnhYUF13HCIiK7Y2bPuPvCdtdpEYGIiIyNQkVERMZGoSIi\nImOjUBERkbFRqIiIyNgoVEREZGwUKiIiMjYKFRERGRuFioiIjI1CRURExkahIiIiY6NQERGRsVGo\niIjI2ChURERkbBQqIiIyNgoVEREZG4WKiIiMjUJFRETGRqEiIiJjo1AREZGxUaiIiMjYKFRERGRs\nFCoiIjI2ChURERkbhYqIiIyNQkVERMZGoSIiImOjUBERkbFRqIiIyNjsWaiY2d1m9nkz+5qZPWtm\nf78pP2ZmT5nZt5qvR5tyM7OPmNlpM/uymb1l5L0ebq7/lpk9PFL+/Wb2leY1HzEz26vvR0REtreX\nLZUS+Ifufh/wAPA+M7sP+CDwOXe/F/hc8xjgncC9ze0R4KMQQgj4EPADwFuBD20EUXPN3x553YN7\n+P2IiMg29ixU3P2cu/9hc38N+DpwF/AQ8PHmso8DP9rcfwj4hAdfAObN7E7gHcBT7r7k7svAU8CD\nzXOH3f0L7u7AJ0beS0REJuCmjKmY2T3AnwO+CNzu7ueap14Bbm/u3wW8NPKyM03Z9crPbFG+1ec/\nYmanzOzU4uLi6/peZGvuzrCoGBYVIeNFZBrteaiY2RzwaeCn3X119LmmhbHnv4Hc/VF3X3D3hRMn\nTuz1x00dd+diL2dlULAyKLjYyxUsIlNqT0PFzFJCoPx7d/9PTfH5puuK5uuFpvwscPfIy082Zdcr\nP7lFudxkw6Kmqi+HSFU7w6KeYI1EZFL2cvaXAb8GfN3d/8XIU08AGzO4HgY+M1L+nmYW2APAStNN\n9iTwdjM72gzQvx14snlu1cweaD7rPSPvJTdRvUWrxPe+ASoit6BkD9/7B4GfBL5iZl9qyv4x8AvA\np8zsvcALwI83z30WeBdwGugDPwXg7ktm9vPA0811P+fuS839vwt8DOgCv93c5CbrpDG9vGQjW8yg\nncSTrZSITIRNW9/3wsKCnzp1atLVOHCq2hkUFQDdNCaOtGRI5CAxs2fcfWG76/aypSIHWFZWDPMw\nbjLTjknjiLm2fpxEpp1+C8iuFVXNpX6x+TgrK26ba6t1IiLa+0t2b9h0c21wQrCIiChUZNe2apFE\n2nZNRFCoyA3opmEMZUM7ieikmu0lIhpTkRtgZhybbVFUNQYksf42EZFAoSI3LFWYiMhV9FtBRETG\nRqEiIiJjo1AREZGxUaiIiMjYKFRERGRsFCoiIjI2ChURERkbhYqIiIyNQkVERMZGoSIiImOjUBER\nkbFRqIiIyNgoVEREZGwUKiIiMjYKFRERGRuFioiIjI1CRURExkahIiIiY6NQERGRsVGoiIjI2ChU\nRERkbBQqIiIyNnsWKmb2mJldMLOvjpT9rJmdNbMvNbd3jTz3M2Z22sy+aWbvGCl/sCk7bWYfHCl/\no5l9sSn/DTNr7dX3IiIiO7OXLZWPAQ9uUf4v3f3+5vZZADO7D3g38Gea1/yKmcVmFgO/DLwTuA/4\nieZagH/WvNefApaB9+7h9yIiIjuwZ6Hi7r8HLO3w8oeAx909c/dvA6eBtza30+7+nLvnwOPAQ2Zm\nwA8D/7F5/ceBHx3rNyAiIrs2iTGV95vZl5vusaNN2V3ASyPXnGnKrlV+G3DJ3curyrdkZo+Y2Skz\nO7W4uDiu70NERK6ybaiY2ed2UrZDHwW+G7gfOAf88xt8n11x90fdfcHdF06cOHEzPlJEZCol13rC\nzDrADHC8aVFY89RhrtMquB53Pz/y/r8K/Jfm4Vng7pFLTzZlXKP8IjBvZknTWhm9XkREJuR6LZW/\nAzwD/Onm68btM8Av3ciHmdmdIw9/DNiYGfYE8G4za5vZG4F7gT8AngbubWZ6tQiD+U+4uwOfB/56\n8/qHm3qJiMgEXbOl4u4fBj5sZn/P3X9xt29sZp8E3kZo6ZwBPgS8zczuBxx4nhBcuPuzZvYp4GtA\nCbzP3avmfd4PPAnEwGPu/mzzER8AHjezfwr8EfBru62jiIiMl4U/+re5yOzPA/cwEkLu/om9q9be\nWVhY8FOnTk26GiIi+4qZPePuC9tdd82Wysgb/TvC4PqXgKopdmBfhoqIiOydbUMFWADu8500aURE\nZKrtZJ3KV4E79roiIiKy/+2kpXIc+JqZ/QGQbRS6+/++Z7USEZF9aSeh8rN7XQkRETkYtg0Vd/8f\nN6MiIiKy/+1k9tcaYbYXQAtIgZ67H97LiomIyP6zk5bKoY37ze7ADwEP7GWl5OBxd2qHOLLtLxaR\nfWtXuxR78FvAO7a9WKSRlRWL6xmvNreyqiddJRHZIzvp/vprIw8jwrqV4Z7VSA6c1UHJxiqnqnbW\nhiVHZ3VQp8hBtJPZX3915H5J2LProT2pjRw4odvrynWzRa2WishBtZMxlZ+6GRWRg8nMSOOIYqTL\nq53EE6yRiOylnRzSddLMftPMLjS3T5vZyZtROTkY5rspnSQmjoxuK+ZwZycNZBHZj3YyUP/rhPNO\nvqO5/eemTGRHosg4MpNyfK7N4U5KmEQoIgfRTkLlhLv/uruXze1jgM7kFRGR19hJqFw0s79pZnFz\n+5uE43zlgMnLmn5eXjH+ISKyGzsJlb8F/DjwCnCOcISvBu8PmH5ecmFtyMuXBry01Gd9WEy6SiKy\nD+1k9tcLgHYkPuCWejmX+peD5OVLQ958RzrBGonIfrSTxY9vBP4erz1OWEFzgAyL6orHRV2TlzWt\nZFebLojIlNvJ3M7fAn6NMOtLne0H1EwrYVjkm4+7aYy26RKR3dpJqAzd/SN7XhOZqDccalPVTl5W\nJHHE/EyLJFYrRUR2Zyeh8mEz+xDwO1x58uMf7lmt5KZL4oiTR7tkZU1kpm4vEbkhOwmV/xX4SeCH\nudz95c1jOUDMjE6qLVRE5MbtJFT+T+BN7p5ve6WIiEy1nfRxfBWY3+uKiIjI/reTlso88A0ze5rL\nYyru7tr+XkRErrCTUPnQyH0D/iLw7r2pjoiI7Gfbdn+5+/8AVoG/AnyMMED/r/e2WiIish9ds6Vi\nZm8GfqK5vQr8BmDu/kM3qW5ykxVVTVbWxGZ00khb1IvIrl2vpfINQqvkr7j7X3D3XwSq61x/BTN7\nrDnU66sjZcfM7Ckz+1bz9WhTbmb2ETM7bWZfNrO3jLzm4eb6b5nZwyPl329mX2le8xHTb8DXJSsr\nlno5vaxkdVhcsQ+YiMhOXS9U/hphV+LPm9mvmtmPEMZUdupjwINXlX0Q+Jy73wt8rnkM8E7g3ub2\nCPBRCCFEGNP5AeCtwIc2gqi55m+PvO7qz5JtDPKKC2tDLqwOOb+aXfFcXtWU2gJfRHbpmqHi7r/l\n7u8G/jTweeCngTeY2UfN7O3bvbG7/x6wdFXxQ8DHm/sfB350pPwTHnwBmDezO4F3AE+5+5K7LwNP\nAQ82zx129y+4uwOfGHkv2YGqdlaHBe5hJeuwqBjk5RXXqPEnIru1k4H6nrv/B3f/q8BJ4I+AD9zg\n593u7uea+68Atzf37wJeGrnuTFN2vfIzW5RvycweMbNTZnZqcXHxBqt+sFx9ENdMK6YcKdo4U15E\nZDd2tcGTuy+7+6Pu/iOv94ObFoa/3vfZ4Wc96u4L7r5w4oROQgZIr9osMo0j7jjc4VAn4Ug35ciM\nzlIRkd272bsGnm+6rmi+XmjKzwJ3j1x3sim7XvnJLcplh+LIONJNiSPDLLRU5joJM61E+3+JyA27\n2aHyBLAxg+th4DMj5e9pZoE9AKw03WRPAm83s6PNAP3bgSeb51bN7IFm1td7Rt5LdqiTxhyfa/OG\nQx0OddQyEZHXbycr6m+ImX0SeBtw3MzOEGZx/QLwKTN7L/AC8OPN5Z8F3gWcBvrATwG4+5KZ/Tzw\ndHPdz7n7xuD/3yXMMOsCv93cRERkgiwMbUyPhYUFP3Xq1KSrISKyr5jZM+6+sN11OolJRETGZs+6\nv2R6DIuKfh42W5hpxdcd6Hd3enlFVoRji+faiaYuixwgChV5XYqqZmVweUuXlUFNEtk1z7dfz8rN\nACrrirKquW2ufVPqKiJ7T91f8rpk5Wu3ctmq7FrPlbVT1dM1ridykKmlMqXKqmZY1hjQTWOiG+yC\nSrZ43fW6s5LIrggRM1Dvl8jBoZbKFCqrenNH4vWs5GIvZ7tZgO5OUdWvua6TxnSSy2Mo3W3GVEbH\nUAw43Em1x5jIAaKWyhQaFNUV++PU7mRlfc0wyMqKlUHYfNIMjnRT2iNBcmQm5VAdfpS2a/EkccTx\nuTZlVTer+RUoIgeJWipTaLe/yNeGJRsNFPfw+GpRZLvqQktiHQImchCppTKFumnMIK+om6RIIqOd\nvPbvi34eZmotrmWbU4Wr2lkfFkQWXjPbvvEfIXdXsIgcMAqVKRRHxvG5Fv2sYlhWxJG9pvsrL+um\nheIkkbE2LIkjY31YEsdGUdVhjIUwTrIbZTMNuazDex/pptecgiwi+4tCZUqZGVlVU9ZOWfvmVN9W\nHJFXNZf6OUu9nNohjcKZ9VXlpLFd0TrJimozVIZFRVbUxLExc50ZZRuBAmFK8eqw5Nhsa4+/YxG5\nGRQqU6psWhqjVptFjLU7L18a0MtLOmlMGUccilNuP9JmdVBeMci/MZNrkFesDptFkGVo6VwrKMqr\n1qXo2GKRg0OhMqW2GstY6uVUzdThS/2cYVlTVU4cGYc6KZ00wd1YGxY4EJlttlIGRXXFexXNGfdb\ndWtttIY2XH1gmIjsXwqVKRVHxkwr3twypWi6wgConfWsZKaVMD/TIo6MTjOQ323FtJOIyv2KMLi6\np8sIobOVw92UtWFBXtW04khnuYgcIAqVKRZaH2FGV9G0StayEsyYa6ckkdFKItpJzEzr8iB+FBkR\nlwOjqGq6aUxe1ZtTj2fayWvGVOraMQuBNj+jMRSRg0ihMuXSOCKNQ6ui04rBIK9qzODoTIt2GmOE\nkLhaVTvL/Zyq9nBNKyaJo80NJYuqZn1YUtQ1eVmTxhFmYRV90sw4iyPT8cUiB4hCZUq5h1ZJVoRf\n7HPthEOdhMiMGeCu+S6RhX262km05djIelZu7uPlQD+vOD4XWijuzqV+Qe3O+rBkUFTMtmJm2gmv\nrmUkkWFNSyYrao7MqAtM5CBQqEypXl4xaMZT6sq5NMg5MddmphV+JNydQVFRuVO5b/mDUl81i8uB\nyp0Io6h8c3HlRvDkVc0M0MtKsLDlSxpFuDtztc5VETkINO1mShVXbUHvDkV1OSRWBgVrw5JBXnGp\nX2wG0Kh2euWPTxwZaRxRN4sajctTl7NmkSVAr6jIy5q8DAd2rWev3fZFRPYntVSmVBIbozlhXN7G\nvhpZDLmhn5d0W1eOfWy0arKiJoqMNDYW1zLqZhV+bMZiP0w/NsAwWnHEsW7K+siHu19/u3wR2T8U\nKlNqtpVshocZHGolDMuKyIy0aWWMdm5da3rwTCthYyLXRqBAWODYzytum2tRj4TG4W5KWTtpEm0O\n1L+e/cNE5Nai/81TKmqm9bo7tcPFXrY5HThptmUZFKG1EmZ/bT1Dq6jqsJV+HRZNjrY4yqrGWjHx\nSB4ZcKiTsDrwsFMxu987TERuXfrfPOXMjN6wIMsrsmaVezeNmW236aShNdNKoi27p4qqZrmXb7Zo\n1oclh7vJ5mr9+ZmU0bH8TrMfWCeKNw/9mm0lxFpRL3JgKFSmmLuzMihYXMt4ZWVAO42ZaSVkZc38\nTIvONVonG4ZXHfY12wnXl826lG4ak8bR5pTkjfUoy718c5uWrCy4bbZ1w8cZi8itRX8iTqmyClvb\nrw1LhkVFUTuDvKKs6mb8ZOvjhevaN48U3miRuHt4j9KZayekccRMKyFvFj9WtW8GSlZWV+z7VbvT\nL147s0xE9ie1VKbMxqLEvKo5tzJgUFSkUfjbYmN1+5GZlDi68u+NqnaW1jOWmhX08zMtjs20iMxY\n7hf08pKyrullBd1WTFk7eRnCJ6sqZtsJSRxtjtuMqrcqFJF9SS2VKTMsavKqpvawODEvasycdhwR\nmW2eP3/1SZBrw4JLw4KicqraOb86ZHE941A7Jk0Mx+mmCbXD+ZWMXna59RFHEb28Cnt/AaMdXUYY\nwxGRg0EtlSlT1pe7nlpxzGwbYjOOH0rppjHH5lq0k9f+ki8qp6xC19elXs5aVtLLQtdZEhmtOLzG\nzJhpJZR1TR055k4SxWRFxfqwCFvke9jMMorCV219L3JwKFSmTDsJ291HFqYNG3B0Nmxvf6Sb0ooj\nVodF2AAyipjrhO1T2mlEEhu9rGJ5EM6odw/rXMqm6dHPS7Ki4nA3JTZjtZ9TOaxlJYc7KWaGA3kZ\nzr2/5/gsK4MCd16zsFJE9qeJ/IloZs+b2VfM7EtmdqopO2ZmT5nZt5qvR5tyM7OPmNlpM/uymb1l\n5H0ebq7/lpk9PInvZb9pJdFmeByfa3PX0S7dNGa+G7bBX8vC1ixV7aznBS9c7IVFjbVzYrZFEkdU\ndd1MCXb6ecmhTsKRbtgQ8lA3pZsmDPKSKIrotmK6acyFteHmeS39vGrGXEKraS0rJvbvISLjNcl+\nhx9y9/vdfaF5/EHgc+5+L/C55jHAO4F7m9sjwEchhBDwIeAHgLcCH9oIIrm+ThpzdDZ0c+VlzbCo\nePnSgMW1ISv98Au+qGouruchAKo6rLyPIu49cYjjc+3QGomMrKiaPb+Mw52UVhTRSsJukWkcusLS\nJMYsHDFcNbPHWkl0eXBF4/QiB8at1P31EPC25v7Hgd8FPtCUf8LDPNYvmNm8md3ZXPuUuy8BmNlT\nwIPAJ29utfefXlaynhVcXM+ZacVkZVhXUlQO5iz1wtYty72cVhxxW3PWfF7WxGnEkZmUV9dysrJi\nphVzYS2nm0Ys9TKKymlnMYOipGsxtYeB/cPtFI+gn5V0WjGH2snmZIC2BupFDoxJhYoDv2NmDvwb\nd38UuN3dzzXPvwLc3ty/C3hp5LVnmrJrlb+GmT1CaOXwnd/5neP6HvalrKyac1DC4PvLKwPW+iU1\nThJF3D3fYVBUzLUT4tiIIlju58y2Ew53kmY8Joy1+MDDqvuq4mwvY1hU1B5C69hci3YcMSwquknM\n4UNhD7B+VmGRcWKuQ5JEm60ZETkYJvW/+S+4+1kzewPwlJl9Y/RJd/cmcMaiCa1HARYWFqa6s2Vj\ne/uiCmtKXl4Z8upaRhxDK4p45dKAu47NcGKuTVnGnF0ZstTLuf1wh3Yc4YS9wVYGBXlRM9NySgun\nOCZxRGRh2nIniak9nAa5uDokr1PWs5JhXnF0tsWx2ZpOFCtQRA6YifyPdvezzdcLZvabhDGR82Z2\np7ufa7q3LjSXnwXuHnn5yabsLJe7yzbKf3ePq77vpc3ujr2sDEcEu4PD8npOVRtpDJ12HA7mimGl\nl9NpxSz3c26ba9PLymZwPiaLI2ogxYhwsjycm1K7c24FumlCHIfW0SvLBXntmBnpsGQtK0njSDsU\nixwwN32g3sxmzezQxn3g7cBXgSeAjRlcDwOfae4/AbynmQX2ALDSdJM9CbzdzI42A/Rvb8rkOtpJ\nvPmLPI0jjsy06LYielnJ6iCjlxf0soIzyz2eW1ynlUa4wQuv9vn904u8cLHHSr9gtV/SGxSs9DPO\nr/UZNtuvxFHEelaxOiiocV65NORiP+Ol5QFnl/sM8yqcKplVVHW9TW1FZL+ZxJ+JtwO/2ewblQD/\nwd3/m5k9DXzKzN4LvAD8eHP9Z4F3AaeBPvBTAO6+ZGY/DzzdXPdzG4P2cn1z7YSTx2YY5BVFUfON\ncytkZcWwrOllJXVtnDzaZVhW1FVOBQyzkn5uzLQiLq6VXOrlDIqKbqfF4U5M5aHR001jvmO+w7B0\nsrzCzKmrcB5LO00o6rBP2OL6kGOzKUu9nPluqg0lRQ6Imx4q7v4c8H1blF8EfmSLcgfed433egx4\nbNx1nAaHmq6vKIL5dkp5pMO3zvfoZyUWG931iPmZlGFRcn41x6nppAmrw4LzK0PK0qmB2XbGvbcf\npp0aSRzTjiOyoma2E7PWL1gelLRj47ZDKXkFM7GxNgxdX+EI45peXnKok076n0RExkAd2lNqWNSs\nZSX95hyVlUFJbIRZXs1033D4FlR1RRTFVHXFy8sZi+sZncQ41G2x2MuoLqzyPccPQVSxMoDeoKKX\nF1gU1sQc66YMi5q75ju8vDJgWNQUtVP6CvfdeeQ1+4yJyP6lUJlSS/2MV1aGLA0y3AyvQ0skMudQ\nO+FiP6OojMOdhFYUVsTnXrPeL8jzmrwy1rMh3VbEsF9yYT1jfjYhiSIqrxlWNUltVGXJ6qCknRjn\nlgcc7qQQweogp67h3Eyf2w9rzarIQaFQmUJZWXFhJePl5R7nVoZcWMtIoogjXePoTMpSP+f8Sk5e\nO8t9WB9WlEVNnECFk9cVc+0Wg2GJGczOp7QT6A1L5loJrTiiFcUUVUm/rOmkEUe6XdaygheW15nt\npAyGFXPdhDcdn6GfV3TSePN8FhHZvxQqU6iflZw+v8I3L6yzOihYG5TcfrjDd8y3udTPOHepz7Co\nyIuKi3lF6ZCaE9fGsHBqr/GqZqYThcWOwyKsoG/FZCVY5LRTGJYwyEu8ThiWJWv9gvOrOd1BSRIb\nFc5aFk6BzMp68yAvEdm/FCpT6MWLfc5cGjLIK4Z5RVHVFHXNsKw5uzLk0iDs97WeV+RlheNUGJ5D\nbU43SWm3IrLSKfKai5YRxzF3xDHH51Iurues5yXtyLhYVZjBi69WDIsKi5y8quhnkMQRLy4NuW22\nx+GTCaBQEdnvFCpTaGWYkxc1/azi1fUhvawiNWM9KxkUNRHhF39R1pSVkyZGXUNeO63YaCVQu1GU\nFe1WhNdGTVj4GOHcOd8lieH5xXUOtRJWhiVZWYft883I8oqsCmtVLqz2ubDapaqmeqMDkQNDoTKF\nEouocXpZwUo/Zy2vaaeGDWLAyaqaXlaRlwVpMz5SWo1ZRBw5URzRiiDqpOSV49REwFw75mK/oCwL\nkgieXx4wzMMhXkkUYTVYBOt52Nk4isK2MTVOroWQIgeCQmUK1V6zNihYHeasZhXtJGJYOJWX1GVJ\nZeHMlDSJSZtNIQt32pGT5xG9umTFa2ZaMZE5JYbFRu1Q17AyyOnnBUvrBevDfHMfsNl2zPxMwlw7\nIYmMY3Nt5totapzZltapiBwECpUps9wLQZJGMChqaIJgUIadi2OHJKlxi6grZznLqSrHMaoEZlsx\nUVRRFrAyLEksbF1vSUJW1NQU5GXBxX5Bb1gwzCvquqaVxBSlszoIW1S7R2RlybG5hHuOzWmtisgB\noVCZMquDgrwsWezlrA8LsrKmn5fMphGz3ZTD7YTVrGSQl/SygrI2WqmREE5qTCxipapJLSKOwJOY\nKIppx/DyyoDVYYHXjkXg7qRxFMZLImdYOL0so6hhrh0xP4jx2jk539U2LSIHhEJlytQ4i6s5ZVFT\nFDVFWQM1g6LmSDfmUi9naZBvtmDi2DFisqqkcsiKAsygBcOsJi5riroiK4zVQU1e1dQediOmrokN\nDGNQOFVNszULrA8Nc7hnfY7lYc6JI51J/9OIyBgoVKbMbBKzOig4szpkNavIinDKo8ewuF5Q1YSp\nwlVz2m8NpZXUNdQG7Qhqd3pZRWwQxWHjyF7lFLXTjiLiKGJYVmFX/cgo3SkLKMPb4UCdOYvrGd94\nZZ0/ubDKm28/PNF/FxEZD4XKlOmXFbPtZLM7K6/DL/qqhmFZE0WhNVGF4RYig8rC4zSGwoEaaocq\ngiSuaUcxmddEBoU7kdfh9RXu3t4jAAAJoUlEQVSUOBhUzedvTBzOgHbpDPOCxZVwNHE70ToVkf1O\noTJlIosoa6df1jT5gAOZN3eumtnbccAgjqCswiUlkAKpQz+DARVJc95XVXlo1TRv5xC2xd+iLsMS\nLqzldFKjrGqFitw8VRm6cSP9zI2bQmXKpInTz0uGeUlVbf3LflQOYX0JISg2/gtGQDYSHmkeyq15\nXHE5VK6ldKi85k9eWVegyM3hDoNlKLPwuDUDnSOTrdMBo3mcU8YwFleH1B4CYzs1ISA2xkOKpjzj\ncnDQlBeE9yy4HDbbvfcgL3m5l9EfFttcLTIGRf9yoADkfSh38j9BdkqhMmXW+wXnlvusDctJVwWA\nvHT6g4K1vNr+YpHXq97i574uIFuD3kUYroRpj3LDFCpTZnmQ8+KlPoNbpGGwnoPhzLTU/SU3QXLV\n1HUzqHLI1sPXvA/DS5Op2wGhMZUp8+3FHlnlV4/HT0wN9LKKmZZ+FOUmSNrQnYdiEB635l4bInkf\n4lbYqC7thuCRHdP/5ClT1hBV9S0TKgBnV/qsZyVtnacie63Mw7hK3ocoCeHhFrrFogTqCrKVy7PC\nij7M3KZg2QWFypQ5cajFWn4rRQq8uFJydmmN2+bak66K3OrqCqoihEG0y977qoT187B2vmmdWAiM\nKAqh4UCcQjo78poiDOyn2vFhpzSmMmXW8pz1W2OM/grffHll0lWQW13eh/ULYUpw78KVs7h2ohyE\nsZPhSnhtXcLqyyGoOkegeyR0dyX64+b1UEtlyjz3yq35y/vF5cGkqyC3umzt8n338Hg3AWBRaKFk\nq6EF4h7WqXgdAqZq/tqKW5dfEyUKmV1SqEyZpdVb85d3N721uuTkFuN+uevLLHRT+S5/ZqIU4k5o\njdQlxE1gJG0YrIawsQiGq3DojjCg35rVeMouKVSmzIuv9iddhS0NhgoVGVEMwhTfuB3GM9yh6MFg\nJZSnM3D8uyHvhZv75cDBIOmCF+F+nEL7MOBw+A5oz4buLzPoHAUv4dJLsHoeyl4Im7IP8feEz1Go\n7IpCZcr88Su3ZkvlG+cWJ10FuVUMV2Dl5TALyyKY/67QTRUlIVC8gnIYBtzrKoRAfxn6FyFfD4sX\n0y50j8HssTBtuC7DuEmUQPtQaIFka2FfoXwI/Vehfx7yQfjMqgrv4U0QyY4pVKbMhVtwkB7g62e1\nVYYQAuHSS7D0fGhhRDEs/jEceyNc/DZUzeB8mcPaOZi7I4RK7zwUWWi1JB1YPdtMHe6FFfNxDMfe\nFLrAiiFceiG0ZvJBGMAfrFz+/GIdohb0liBqQ6cJIdkRhYrcEs4MJ10DuSUMlsOMrKIPWR5+qVdD\nuPRiCBuvodWMiczdAdHF0D22diG0LvBwTZFDMgitnqpsusMstHKKYficKocj39GsUUmhdSh0ga2c\nCXVIW6F1019SqOyCQkVEbg1lfnnvrbVz4Zd+XcPF07B+MawnqUtIWmEspL8UxkrSmRA82Xro3oq6\nYbHE6gDMofdqKH/1a+HYUfPLq+iX3wAn/pdQ9upzsPI8XDobur5e/CJYDLPHJ/mvsu/s+1AxsweB\nDxN2Zf+37v4LE66SiNyIugyhkq/Dma/B+llYfSWEgtchVJJuCID24bBIsRqGFkirC/FMGC+Zuw2G\nOZRLcOkc5GshoIiBollBXzdTiCM4cjvMvQEGr8L6q6H7qwYO3QYYfOcDE/1n2W/2daiYWQz8MvCX\ngTPA02b2hLt/bbI1E5Eb8so34fc/HFoNXNUnWgPlcrg/ODe+z1xehOUtyldX4dtD+LP/B9z5veP7\nvANuv6+ofytw2t2fc/cceBx4aMJ1EpEbYnD6t0PX19WBMinDS/DyNyZdi31lv4fKXcBLI4/PNGVX\nMLNHzOyUmZ1aXNTUVZFbkteQ3WLrqOI4dLHJju33UNkRd3/U3RfcfeHEiROTro6IbCXtwMmFZqFi\na9vLb4q52+HeH550LfaVfT2mApwF7h55fLIpk33m+V/43yZdBZm0KIa3/l8wuATP/fewALIcEFYo\nWpj2256FzjzUFtan5GvQWw6r4lvdsL6k6F8+4TGeCSvr6xLwZqA/CptTkhEGahIg5fLh2XWYynzs\nTfCD/w981w9M4l9j3zL37U4Sv3WZWQL8MfAjhDB5Gvgb7v7stV6zsLDgp06dukk1FBE5GMzsGXdf\n2O66fd1ScffSzN4PPEmYL/jY9QJFRET21r4OFQB3/yzw2UnXQ0REpmSgXkREbg6FioiIjI1CRURE\nxkahIiIiY6NQERGRsVGoiIjI2ChURERkbPb1ivobYWaLwAuTrscBcRx4ddKVELkG/XyO13e5+7ab\nJ05dqMj4mNmpnWzbIDIJ+vmcDHV/iYjI2ChURERkbBQq8no8OukKiFyHfj4nQGMqIiIyNmqpiIjI\n2ChU5IaY2YNm9k0zO21mH5x0fUQ2mNljZnbBzL466bpMI4WK7JqZxcAvA+8E7gN+wszum2ytRDZ9\nDHhw0pWYVgoVuRFvBU67+3PungOPAw9NuE4iALj77wFLk67HtFKoyI24C3hp5PGZpkxEppxCRURE\nxkahIjfiLHD3yOOTTZmITDmFityIp4F7zeyNZtYC3g08MeE6icgtQKEiu+buJfB+4Eng68Cn3P3Z\nydZKJDCzTwK/D3yPmZ0xs/dOuk7TRCvqRURkbNRSERGRsVGoiIjI2ChURERkbBQqIiIyNgoVEREZ\nG4WKyB4xszvM7HEz+xMze8bMPmtmb9buuXKQJZOugMhBZGYG/CbwcXd/d1P2fcDtE62YyB5TS0Vk\nb/wQULj7v94ocPf/ychGnGZ2j5n9f2b2h83tzzfld5rZ75nZl8zsq2b2F80sNrOPNY+/Ymb/4OZ/\nSyLbU0tFZG98L/DMNtdcAP6yuw/N7F7gk8AC8DeAJ939/23OrpkB7gfucvfvBTCz+b2rusiNU6iI\nTE4K/JKZ3Q9UwJub8qeBx8wsBX7L3b9kZs8BbzKzXwT+K/A7E6mxyDbU/SWyN54Fvn+ba/4BcB74\nPkILpQWbh0z9JcLOzx8zs/e4+3Jz3e8C/zfwb/em2iKvj0JFZG/8d6BtZo9sFJjZn+XKIwOOAOfc\nvQZ+Eoib674LOO/uv0oIj7eY2XEgcvdPA/8EeMvN+TZEdkfdXyJ7wN3dzH4M+Fdm9gFgCDwP/PTI\nZb8CfNrM3gP8N6DXlL8N+EdmVgDrwHsIJ2v+uplt/CH4M3v+TYjcAO1SLCIiY6PuLxERGRuFioiI\njI1CRURExkahIiIiY6NQERGRsVGoiIjI2ChURERkbBQqIiIyNv8/TQFswxzfedUAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c5ca7cbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.stripplot(x='Class', y='Amount', data=data1, jitter=True, alpha=.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source: https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now\n",
    "#### Goal of this notebook is to recreate the success of the above source notebook via the given resampling method, understand why it works, and to work through the code (specifically learn more about cross validation/model evaluation techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(data['Class'], sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Class  normAmount  \n",
       "0      0    0.244964  \n",
       "1      0   -0.342475  \n",
       "2      0    1.160686  \n",
       "3      0    0.140534  \n",
       "4      0   -0.073403  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'Class']\n",
    "y = data.loc[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points in the minority class\n",
    "number_records_fraud = len(data[data.Class == 1])\n",
    "fraud_indices = np.array(data[data.Class == 1].index)\n",
    "\n",
    "# Picking the indices of the normal classes\n",
    "normal_indices = data[data.Class == 0].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "# Under sampled dataset\n",
    "under_sample_data = data.iloc[under_sample_indices,:]\n",
    "\n",
    "X_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\n",
    "y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of normal transactions:  0.5\n",
      "Percentage of fraud transactions:  0.5\n",
      "Total number of transactions in resampled data:  984\n"
     ]
    }
   ],
   "source": [
    "# Showing ratio\n",
    "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
    "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
    "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions train dataset:  199364\n",
      "Number transactions test dataset:  85443\n",
      "Total number of transactions:  284807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Whole dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Number transactions train dataset: \", len(X_train))\n",
    "print(\"Number transactions test dataset: \", len(X_test))\n",
    "print(\"Total number of transactions: \", len(X_train)+len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions train dataset:  688\n",
      "Number transactions test dataset:  296\n",
      "Total number of transactions:  984\n"
     ]
    }
   ],
   "source": [
    "# Undersampled dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
    "                                                                                                   ,y_undersample\n",
    "                                                                                                   ,test_size = 0.3\n",
    "                                                                                                   ,random_state = 0)\n",
    "print(\"Number transactions train dataset: \", len(X_train_undersample))\n",
    "print(\"Number transactions test dataset: \", len(X_test_undersample))\n",
    "print(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we know, due to the imbalacing of the data, many observations could be predicted as False Negatives e.g. fraudulent transactions incorrectly classified\n",
    "- So the goal will be to increase recall, even at a cost to precision since we don't mind if we have false positives (as long as its not too many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def printing_Kfold_scores(x_train_data,y_train_data):\n",
    "    fold = KFold(len(y_train_data),5,shuffle=False) \n",
    "\n",
    "    # Different C parameters\n",
    "    c_param_range = [0.01,0.1,0.5,1,10,100]\n",
    "\n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "\n",
    "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        print('-------------------------------------------')\n",
    "        print('C parameter: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        recall_accs = []\n",
    "        prec_accs = []\n",
    "        accs = []\n",
    "        for iteration, indices in enumerate(fold,start=1):\n",
    "\n",
    "            # Call the logistic regression model with a certain C parameter\n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n",
    "\n",
    "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
    "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
    "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
    "\n",
    "            # Predict values using the test indices in the training data\n",
    "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
    "\n",
    "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n",
    "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            precision_acc = precision_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            acc = accuracy_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            recall_accs.append(recall_acc)\n",
    "            prec_accs.append(precision_acc)\n",
    "            accs.append(acc)\n",
    "            print('Iteration ', iteration,': recall score = ', recall_acc, 'precision score: ', precision_acc, 'accuracy score: ', acc)\n",
    "\n",
    "        # The mean value of those recall scores is the metric we want to save and get hold of.\n",
    "        results_table.loc[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('   Mean recall score ', np.mean(recall_accs))\n",
    "        print('Mean precision score ', np.mean(prec_accs))\n",
    "        print(' Mean accuracy score ', np.mean(accs))\n",
    "        print('')\n",
    "\n",
    "    best_c = results_table.iloc[results_table['Mean recall score'].astype(float).idxmax()]['C_parameter']\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print('*********************************************************************************')\n",
    "    #print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "C parameter:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : recall score =  0.931506849315 precision score:  0.739130434783 accuracy score:  0.789855072464\n",
      "Iteration  2 : recall score =  0.931506849315 precision score:  0.790697674419 accuracy score:  0.833333333333\n",
      "Iteration  3 : recall score =  0.983050847458 precision score:  0.659090909091 accuracy score:  0.775362318841\n",
      "Iteration  4 : recall score =  0.972972972973 precision score:  0.734693877551 accuracy score:  0.795620437956\n",
      "Iteration  5 : recall score =  0.969696969697 precision score:  0.711111111111 accuracy score:  0.795620437956\n",
      "\n",
      "   Mean recall score  0.957746897752\n",
      "Mean precision score  0.726944801391\n",
      " Mean accuracy score  0.79795832011\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : recall score =  0.849315068493 precision score:  0.984126984127 accuracy score:  0.913043478261\n",
      "Iteration  2 : recall score =  0.86301369863 precision score:  0.969230769231 accuracy score:  0.913043478261\n",
      "Iteration  3 : recall score =  0.949152542373 precision score:  0.982456140351 accuracy score:  0.971014492754\n",
      "Iteration  4 : recall score =  0.932432432432 precision score:  0.985714285714 accuracy score:  0.956204379562\n",
      "Iteration  5 : recall score =  0.909090909091 precision score:  1.0 accuracy score:  0.956204379562\n",
      "\n",
      "   Mean recall score  0.900600930204\n",
      "Mean precision score  0.984305635885\n",
      " Mean accuracy score  0.94190204168\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  0.5\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : recall score =  0.86301369863 precision score:  0.969230769231 accuracy score:  0.913043478261\n",
      "Iteration  2 : recall score =  0.890410958904 precision score:  0.942028985507 accuracy score:  0.913043478261\n",
      "Iteration  3 : recall score =  0.966101694915 precision score:  0.98275862069 accuracy score:  0.978260869565\n",
      "Iteration  4 : recall score =  0.932432432432 precision score:  0.985714285714 accuracy score:  0.956204379562\n",
      "Iteration  5 : recall score =  0.909090909091 precision score:  0.983606557377 accuracy score:  0.948905109489\n",
      "\n",
      "   Mean recall score  0.912209938795\n",
      "Mean precision score  0.972667843704\n",
      " Mean accuracy score  0.941891463028\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : recall score =  0.86301369863 precision score:  0.969230769231 accuracy score:  0.913043478261\n",
      "Iteration  2 : recall score =  0.904109589041 precision score:  0.95652173913 accuracy score:  0.927536231884\n",
      "Iteration  3 : recall score =  0.983050847458 precision score:  0.983050847458 accuracy score:  0.985507246377\n",
      "Iteration  4 : recall score =  0.945945945946 precision score:  0.958904109589 accuracy score:  0.948905109489\n",
      "Iteration  5 : recall score =  0.909090909091 precision score:  0.983606557377 accuracy score:  0.948905109489\n",
      "\n",
      "   Mean recall score  0.921042198033\n",
      "Mean precision score  0.970262804557\n",
      " Mean accuracy score  0.9447794351\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  10\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : recall score =  0.86301369863 precision score:  0.954545454545 accuracy score:  0.905797101449\n",
      "Iteration  2 : recall score =  0.890410958904 precision score:  0.942028985507 accuracy score:  0.913043478261\n",
      "Iteration  3 : recall score =  0.983050847458 precision score:  0.983050847458 accuracy score:  0.985507246377\n",
      "Iteration  4 : recall score =  0.959459459459 precision score:  0.934210526316 accuracy score:  0.941605839416\n",
      "Iteration  5 : recall score =  0.924242424242 precision score:  0.968253968254 accuracy score:  0.948905109489\n",
      "\n",
      "   Mean recall score  0.924035477739\n",
      "Mean precision score  0.956417956416\n",
      " Mean accuracy score  0.938971754998\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  100\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : recall score =  0.876712328767 precision score:  0.955223880597 accuracy score:  0.913043478261\n",
      "Iteration  2 : recall score =  0.876712328767 precision score:  0.941176470588 accuracy score:  0.905797101449\n",
      "Iteration  3 : recall score =  0.983050847458 precision score:  0.983050847458 accuracy score:  0.985507246377\n",
      "Iteration  4 : recall score =  0.959459459459 precision score:  0.934210526316 accuracy score:  0.941605839416\n",
      "Iteration  5 : recall score =  0.909090909091 precision score:  0.967741935484 accuracy score:  0.941605839416\n",
      "\n",
      "   Mean recall score  0.921005174708\n",
      "Mean precision score  0.956280732089\n",
      " Mean accuracy score  0.937511900984\n",
      "\n",
      "*********************************************************************************\n",
      "*********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        1#print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "# dataset\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred_undersample = lr.predict(X_test_undersample.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall metric in the (undersampled) testing dataset:  0.918367346939\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall metric in the (undersampled) testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAEmCAYAAADmw8JdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c5c92dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "# dataset\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred = lr.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall metric in the entire testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVE\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "y_pred_undersample_score = lr.fit(X_train_undersample,y_train_undersample.values.ravel()).decision_function(X_test_undersample.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test_undersample,y_pred_undersample_score)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "# roc_auc_score(y_test_undersample,y_pred_undersample_score) # <-- Same as computing above auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.0])\n",
    "plt.ylim([-0.1,1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having tested our previous approach, I find really interesting to test the same process on the skewed data. Our intuition is that skewness will introduce issues difficult to capture, and therefore, provide a less effective algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = printing_Kfold_scores(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "# dataset\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train,y_train.values.ravel())\n",
    "y_pred_undersample = lr.predict(X_test) # <--- This is really the entire dataset, not an undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred_undersample)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall metric on the entire testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "# OR use recall_score(y_test, y_pred_undersample) from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before continuing... changing classification threshold.\n",
    "We have seen that by undersampling the data, our algorithm does a much better job at detecting fraud. I wanted also to show how can we tweak our final classification by changing the thresold.¶\n",
    "- Initially, you build the classification model and then you predict unseen data using it.\n",
    "- We previously used the \"predict()\" method to decided whether a record should belong to \"1\" or \"0\".\n",
    "- There is another method \"predict_proba()\".\n",
    "- This method returns the probabilities for each class. The idea is that by changing the threshold to assign a record to class 1, we - can control precision and recall.\n",
    "#### Let's check this using the undersampled data (best C_param = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = 0.01, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)\n",
    "\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "from sklearn.metrics import precision_score\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "j = 1\n",
    "for i in thresholds:\n",
    "    y_test_predictions_high_recall = y_pred_undersample_proba[:,1] > i\n",
    "    \n",
    "    plt.subplot(3,3,j)\n",
    "    j += 1\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test_undersample,y_test_predictions_high_recall)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # tp / (tp + fn)\n",
    "    print(\"   Recall: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "    # tp / (tp + fp)\n",
    "    print(\"Precision: \", precision_score(y_test_undersample, y_test_predictions_high_recall))\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    class_names = ['not fraud','fraud']\n",
    "    plot_confusion_matrix(cnf_matrix\n",
    "                          , classes=class_names\n",
    "                          , title='Threshold >= %s'%i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Nicely done. I was unfamiliar with logistic regression, and this helped me gain some insight into the process. One comment I would like to make on the accuracy vs recall. You spent most of your focus on recall and then mentioning that accuracy is important, but not really explaining why. A financial company wouldn't want to miss catching fraud FN which is why recall is important. However, you also have to consider lost accuracy (false positive) is lost money for the company as well, because they may have to call the customer and verify that the purchase was indeed authentic which takes resources. So, while the overall goal is to have perfect recall, I believe a bit more effort is needed in your notebook considering the tradeoff between the two. Is having C=.01 really best because it has the best recall, or is a slightly higher C value a better \"sweet spot\" that sacrifices little on recal, but greatly improves accuracy? Just a thought.\n",
    "\n",
    "As a side note, I used KNN with weighted means and achieve around 91-92% percent recall on my test set using similar undersampling as you. It was interesting to see this perform slightly better on recall. I'll have to check how the two methods compare in accuracy.\" \n",
    "\n",
    "Quote from Brian on <a href=\"https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now#169984\">Kaggle</a>\n",
    "\n",
    "#### Let's implement Brian's idea and see if we can improve the accuracy with a high degree of recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the modified \"printing_Kfold_scores\" function, the origininal best C parameter is very bad for precision. So assuming it is less costly to have 7% less recall (meaning we catch slightly less fradulent payments) and 28% more accuracy/precision (meaning we have a lot less false positives) , then then the best C parameter to choose would be 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
