{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Likelihood-definition:-the-theoretical-probability-of-having-gotten-the-observed-outcomes.\" data-toc-modified-id=\"Likelihood-definition:-the-theoretical-probability-of-having-gotten-the-observed-outcomes.-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>Likelihood definition: the theoretical probability of having gotten the observed outcomes.</a></span></li><li><span><a href=\"#Objective:-$\\hat{\\theta}_{MLE}-=-\\underset{\\theta}{\\mathrm{argmin}}&#10;\\-L(\\theta;-x_1,x_2,....,x_n)$\" data-toc-modified-id=\"Objective:-$\\hat{\\theta}_{MLE}-=-\\underset{\\theta}{\\mathrm{argmin}}\n",
    "\\-L(\\theta;-x_1,x_2,....,x_n)$-0.0.2\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>Objective: $\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\mathrm{argmin}}\n",
    "\\ L(\\theta; x_1,x_2,....,x_n)$</a></span></li><li><span><a href=\"#(conditional)-Likelihood-function-to-maximize\" data-toc-modified-id=\"(conditional)-Likelihood-function-to-maximize-0.0.3\"><span class=\"toc-item-num\">0.0.3&nbsp;&nbsp;</span>(conditional) Likelihood function to maximize</a></span></li><li><span><a href=\"#Link-function:\" data-toc-modified-id=\"Link-function:-0.0.4\"><span class=\"toc-item-num\">0.0.4&nbsp;&nbsp;</span>Link function:</a></span></li><li><span><a href=\"#Log-(conditional)-likelihood-function-to-maximize\" data-toc-modified-id=\"Log-(conditional)-likelihood-function-to-maximize-0.0.5\"><span class=\"toc-item-num\">0.0.5&nbsp;&nbsp;</span>Log (conditional) likelihood function to maximize</a></span></li><li><span><a href=\"#There-is-another-common-way-that-the-LCL-is-written-as-well.\" data-toc-modified-id=\"There-is-another-common-way-that-the-LCL-is-written-as-well.-0.0.6\"><span class=\"toc-item-num\">0.0.6&nbsp;&nbsp;</span>There is another common way that the LCL is written as well.</a></span></li><li><span><a href=\"#Partial-derivative-of-log-conditional-likelihood-to-get-update-rule:\" data-toc-modified-id=\"Partial-derivative-of-log-conditional-likelihood-to-get-update-rule:-0.0.7\"><span class=\"toc-item-num\">0.0.7&nbsp;&nbsp;</span>Partial derivative of log conditional likelihood to get update rule:</a></span></li><li><span><a href=\"#Update-rule:-$$-\\beta_j-=-\\beta_j-+-\\alpha-\\sum_{i=1}^{N}-(y_i---h_{\\theta}(x_i))x_{ij}$$\" data-toc-modified-id=\"Update-rule:-$$-\\beta_j-=-\\beta_j-+-\\alpha-\\sum_{i=1}^{N}-(y_i---h_{\\theta}(x_i))x_{ij}$$-0.0.8\"><span class=\"toc-item-num\">0.0.8&nbsp;&nbsp;</span>Update rule: <script type=\"math/tex; mode=display\" id=\"MathJax-Element-593\"> \\beta_j = \\beta_j + \\alpha \\sum_{i=1}^{N} (y_i - h_{\\theta}(x_i))x_{ij}</script></a></span></li><li><span><a href=\"#This-is-very-important!-Always-start-with-a-$\\-\\alpha-\\leq-.00001$.-The-thought-goes-to-make-sure-that-you-have-converging-behavior-before-finding-a-more-optimal-alpha\" data-toc-modified-id=\"This-is-very-important!-Always-start-with-a-$\\-\\alpha-\\leq-.00001$.-The-thought-goes-to-make-sure-that-you-have-converging-behavior-before-finding-a-more-optimal-alpha-0.0.9\"><span class=\"toc-item-num\">0.0.9&nbsp;&nbsp;</span>This is very important! Always start with a $\\ \\alpha \\leq .00001$. The thought goes to make sure that you have converging behavior before finding a more optimal alpha</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCncRkRT0/wHojPcdrY5zGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f504e17fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('coronary_heart_disease.csv')\n",
    "\n",
    "def s(x):\n",
    "    return 1 / (1 + np.exp(- x))\n",
    "\n",
    "domain = [-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6]\n",
    "plt.plot(domain, [s(a) for a in domain])\n",
    "plt.title('Sigmoid function on domain [-6,6]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>CHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  CHD\n",
       "0   20    0\n",
       "1   23    0\n",
       "2   24    0\n",
       "3   25    1\n",
       "4   25    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('coronary_heart_disease.csv')\n",
    "X = data['Age']\n",
    "y = data['CHD']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood definition: the theoretical probability of having gotten the observed outcomes.\n",
    "\n",
    "So in maximizing likelihood over all choices of theta, you can imagine that we are trying to recreate in our model the probability distribution (which would be Bernoulli in this case) from which the data is realized (because I believe that this would actually maximize the probability of having gotten the data we observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: $\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\mathrm{argmin}}\n",
    "\\ L(\\theta; x_1,x_2,....,x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (conditional) Likelihood function to maximize\n",
    "$$ L(\\theta; x_1, x_2,...,x_n) = \\prod_{i=1}^{N} p_{i}^{y_i}(1 - p_i)^{1 - y_i},$$\n",
    "<br>\n",
    "where $p_i = p(Y = 1 \\mid x_i; \\theta) = \\sigma(\\sum_{j=1}^{M} \\theta_jx_{ij}) = \\dfrac{1}{1 + \\mathrm{e}^{-(\\sum_{j=1}^{M} \\theta_jx_{ij})}}$\n",
    "###### Note that the $\\theta_0$ value is assumed to be included in the vector and $x_0 = 1$\n",
    "###### Note also that the \"where\" line is an assumption; we are assuming that the probability that Y = 1 is a nonlinear function (sigmoid function) of a linear function of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link function: \n",
    "This function, $g$, takes the mean response and returns back the linear combination of predictors and theta. $g$ is the inverse of the sigmoid function. Therefore, $g^{-1}(z)$ will be your prediction (i.e. $p(Y = 1 \\mid X; \\theta)$), where $z$ is the linear combination of theta and your predictors. This is how logistic regression fits into generalized linear models. Note that for generalized linear models use only exponential family distributions to model the response variable. \n",
    "\n",
    "The below equation gives the log odds for a single training example (x value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log\\dfrac{p}{1-p} = \\sum_{j=1}^{M} \\theta_jx_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log (conditional) likelihood function to maximize\n",
    "\n",
    "This can be done because log is a monotonic function (always increasing => x1 > x2 => log(x1) > log(x2)). This means that the same theta that maximizes the log likelihood also maximizes the likelihood. This also makes the likelihood much easier to differentiate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "LCL = \\sum_{i=1}^{N}\\log L(\\theta; y_i \\mid x_i) \n",
    "    = \\sum_{i:y_i=1} \\log p_i + \\sum_{i:y_i=0} \\log (1 - p_i) \n",
    "    = \\sum_{i=1}^{N} y_i\\log p_i + (1-y_i)\\log(1 - p_i) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is another common way that the LCL is written as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Let} \\ z = \\sum_{j=1}^{M} \\theta_jx_{ij} $$\n",
    "$$LLC = \\sum_{i=1}^{N} y_i\\log p_i + (1-y_i)\\log(1 - p_i) $$\n",
    "$$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\sum_{i=1}^{N} y_i\\log \\dfrac{\\mathrm{e}^z}{1+\\mathrm{e}^z} + (1-y_i)\\log(1 - \\dfrac{\\mathrm{e}^z}{1+\\mathrm{e}^z}) $$\n",
    "$$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\sum_{i=1}^{N} y_i\\log \\dfrac{\\mathrm{e}^z}{1+\\mathrm{e}^z} + (1-y_i)\\log(\\dfrac{1}{1+\\mathrm{e}^z}) $$\n",
    "$$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\sum_{i=1}^{N} y_{i}z - y_i\\log(1+\\mathrm{e}^z) - (1-y_i)\\log(1+\\mathrm{e}^z) $$\n",
    "$$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\sum_{i=1}^{N} y_{i}z - y_i\\log(1+\\mathrm{e}^z) - \\log(1+\\mathrm{e}^z) + y_i\\log(1+\\mathrm{e}^z) $$\n",
    "$$ = \\sum_{i=1}^{N} y_{i}z - \\log(1+\\mathrm{e}^z) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_X_train = np.insert(X_train.values.reshape(-1,1), 0, 1, axis=1)\n",
    "fixed_y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(theta, X_train, y_train):\n",
    "    # eps is to prevent log(0) \n",
    "    eps = .000000000000000000000000000000000001\n",
    "    s = 0.0\n",
    "    for i in range(len(X_train)):\n",
    "        p = sigmoid_hypothesis(theta, X_train[i, :])\n",
    "        y_is_one = y_train[i]*np.log(p+eps)\n",
    "        y_is_zero = (1-y_train[i])*np.log(1-p+eps)\n",
    "        s += y_is_one + y_is_zero\n",
    "    return s\n",
    "\n",
    "def sigmoid_hypothesis(theta, x):\n",
    "    return 1 / (1 + np.exp(-np.dot(theta, x)))\n",
    "\n",
    "def compare_with_sklearn_lgr(theta, X_train, y_train):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(fit_intercept=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    sklearn_theta = clf.coef_.flatten()\n",
    "    print('sklearn theta', sklearn_theta, 'sklearn likelihood', likelihood(sklearn_theta, \n",
    "                                                                           X_train,\n",
    "                                                                           y_train))\n",
    "    print('scratch theta', theta, 'scratch likelihood', likelihood(theta,\n",
    "                                                                   X_train,\n",
    "                                                                   y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial derivative of log conditional likelihood to get update rule:\n",
    "$$ \\frac{\\partial}{\\partial \\theta_i}LLC = \\frac{\\partial }{\\partial \\theta_i}\\sum_{i=1}^{N} y_{i}z - \\log(1+\\mathrm{e}^z) $$\n",
    "$$ = \\sum_{i=1}^{N} y_{i}x_{ij} - \\dfrac{1}{1+\\mathrm{e}^z}\\frac{\\partial}{\\partial \\theta_i}1+e^z $$\n",
    "$$ = \\sum_{i=1}^{N} y_{i}x_{ij} - \\dfrac{e^{z}x_{ij}}{1+\\mathrm{e}^z} $$\n",
    "$$ = \\sum_{i=1}^{N} (y_{i} - h_{\\theta}(x_i))x_{ij} $$\n",
    ", where $h_{\\theta}(x_i) = \\dfrac{1}{1+\\mathrm{e}^{-\\sum_{j=1}^{M} \\theta_jx_{ij}}}$\n",
    "\n",
    "### Update rule: $$ \\beta_j = \\beta_j + \\alpha \\sum_{i=1}^{N} (y_i - h_{\\theta}(x_i))x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is very important! Always start with a $\\ \\alpha \\leq .00001$. The thought goes to make sure that you have converging behavior before finding a more optimal alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn theta [-1.64784075  0.06906915] sklearn likelihood -57.7476896657\n",
      "scratch theta [-0.14083378  0.00329964] scratch likelihood -45.7770069369\n"
     ]
    }
   ],
   "source": [
    "def gd_element_wise(theta, X_train, y_train, alpha=.00001):\n",
    "    errors = np.zeros([len(y_train)])\n",
    "    for m in range(len(y_train)):\n",
    "            errors[m] = \\\n",
    "            sigmoid_hypothesis(theta, \n",
    "                               X_train[m, :]) - y_train[m]\n",
    "    for j in range(len(theta)):\n",
    "        # This is the sum next to alpha \n",
    "        gradient_j = 0.0\n",
    "        for m in range(len(y_train)):\n",
    "            gradient_j += errors[m] * X_train[m, j]\n",
    "        # Note that we subtract here because gradient has a negative value\n",
    "        # so we go in opposite direction to maximze ???\n",
    "        theta[j] = theta[j] - alpha * gradient_j\n",
    "\n",
    "def pretrain(X_train, y_train, add_intercept=True):\n",
    "    if y_train.ndim != 1: \n",
    "        raise Exception('y_train should be 1D')\n",
    "    y_train = np.array(y_train)\n",
    "    if X_train.ndim == 1: \n",
    "        X_train = np.array([np.array([x]) for x in X_train])\n",
    "    # Add intercept for x_0 = 1 \n",
    "    if add_intercept:\n",
    "        # insert syntax: arr, index, value, dimension to insert\n",
    "        # it will insert for each array in the given dimension\n",
    "        # i.e. it is broadcasted \n",
    "        X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "        \n",
    "    return [X_train, y_train]\n",
    "        \n",
    "def train_element_wise(X_train, y_train, max_iter, alpha=.00001, \n",
    "                      add_intercept=True, showConverge=False):\n",
    "    X_train, y_train = pretrain(X_train, y_train, add_intercept)\n",
    "        \n",
    "    # length of theta should be === # features \n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    for i in range(max_iter):\n",
    "        gd_element_wise(theta, X_train, y_train, alpha)\n",
    "        if showConverge:\n",
    "            print('likelihood', likelihood(theta, X_train, y_train))\n",
    "    return theta\n",
    "\n",
    "theta = train_element_wise(X_train, y_train, 1000, .00003, showConverge=False)\n",
    "compare_with_sklearn_lgr(theta, fixed_X_train, fixed_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn theta [-1.64784075  0.06906915] sklearn likelihood -57.7476896657\n",
      "scratch theta [-0.14083378  0.00329964] scratch likelihood -45.7770069369\n"
     ]
    }
   ],
   "source": [
    "# some vectorization for better performance\n",
    "def h_vec(theta, X):\n",
    "    # -X @ theta is matrix multipy\n",
    "    return 1 / (1 + np.exp(-X @ theta))\n",
    "\n",
    "def gd_better(theta,  X_train, y_train, alpha=.00001):\n",
    "    # Predictions for all examples\n",
    "    predictions = h_vec(theta, X_train)\n",
    "    errors = predictions - y_train\n",
    "    for j in range(len(theta)):\n",
    "        # Note that we subtract here because gradient has a negative value\n",
    "        # so we go in opposite direction to maximze ???\n",
    "        theta[j] = theta[j] - alpha * np.dot(errors, X_train[:, j])\n",
    "        \n",
    "def train_better(X_train, y_train, max_iter, alpha=.00001, add_intercept=True, showConverge=False):\n",
    "    X_train, y_train = pretrain(X_train, y_train, add_intercept)\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    for i in range(max_iter):\n",
    "        gd_better(theta, X_train, y_train, alpha) \n",
    "        if showConverge:\n",
    "            print('likelihood', likelihood(theta, X_train, y_train))\n",
    "    return theta\n",
    "\n",
    "theta = train_better(X_train, y_train, 1000, .00003, showConverge=False)\n",
    "compare_with_sklearn_lgr(theta, fixed_X_train, fixed_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood -46.43788095\n",
      "sklearn theta [-1.64784075  0.06906915] sklearn likelihood -57.7476896657\n",
      "scratch theta [-0.14083378  0.00329964] scratch likelihood -45.7770069369\n"
     ]
    }
   ],
   "source": [
    "# fully vectorized - Adapted from http://cs229.stanford.edu/section/vec_demo/lr.ipynb\n",
    "def gd (theta, X_train, y_train, alpha=.00001):\n",
    "    predictions = h_vec(theta, X_train)\n",
    "    errors = predictions - y_train\n",
    "    # Note that we subtract here because gradient has a negative value\n",
    "    # so we go in opposite direction to maximze ???\n",
    "    theta -= alpha * errors @ X_train\n",
    "    \n",
    "def train_vec(X_train, y_train, max_iter, alpha=.00001, add_intercept=True, showConverge=False):\n",
    "    X_train, y_train = pretrain(X_train, y_train, add_intercept)\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    for i in range(max_iter):\n",
    "        gd(theta, X_train, y_train, alpha)  \n",
    "        if i % 10000 == 0:\n",
    "            print('likelihood', likelihood(theta, X_train, y_train))\n",
    "    return theta\n",
    "\n",
    "theta = train_vec(X_train, y_train, 1000, .00003, showConverge=False)\n",
    "compare_with_sklearn_lgr(theta, fixed_X_train, fixed_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
