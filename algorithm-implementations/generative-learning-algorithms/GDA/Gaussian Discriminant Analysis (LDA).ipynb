{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You will need to unzip pima-indians-diabetes-database.zip for this code to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#The-model:\" data-toc-modified-id=\"The-model:-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>The model:</a></span></li><li><span><a href=\"#Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):\" data-toc-modified-id=\"Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):-0.0.2\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>Log-likelihood of the data (b/c X is assumed iid):</a></span></li><li><span><a href=\"#So-maximizing-this-gives-us-the-maximum-likelihood-estimates-for-our-parameters.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" data-toc-modified-id=\"So-maximizing-this-gives-us-the-maximum-likelihood-estimates-for-our-parameters.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf-0.0.3\"><span class=\"toc-item-num\">0.0.3&nbsp;&nbsp;</span>So maximizing this gives us the maximum likelihood estimates for our parameters. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: <a href=\"http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" target=\"_blank\">http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf</a></a></span></li><li><span><a href=\"#Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:\" data-toc-modified-id=\"Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:-0.0.4\"><span class=\"toc-item-num\">0.0.4&nbsp;&nbsp;</span>Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:</a></span></li><li><span><a href=\"#Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probable,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y\" data-toc-modified-id=\"Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probable,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y-0.0.5\"><span class=\"toc-item-num\">0.0.5&nbsp;&nbsp;</span>Since are classification strategy will involve choosing the class that is most probable, we do not need to calculate the denominator of Bayes's rule because it is independent of y</a></span></li><li><span><a href=\"#However-it-is-good-to-know-that-the-denominator-is-given-by:\" data-toc-modified-id=\"However-it-is-good-to-know-that-the-denominator-is-given-by:-0.0.6\"><span class=\"toc-item-num\">0.0.6&nbsp;&nbsp;</span>However it is good to know that the denominator is given by:</a></span></li><li><span><a href=\"#Finally,-the-other-(actually-imporant)-quantities-in-Bayes's-rule-are-given-by-our-model-(i.e.-we-assumed-X-is-distributed-Gaussian-and-Y-is-Bernoulli)\" data-toc-modified-id=\"Finally,-the-other-(actually-imporant)-quantities-in-Bayes's-rule-are-given-by-our-model-(i.e.-we-assumed-X-is-distributed-Gaussian-and-Y-is-Bernoulli)-0.0.7\"><span class=\"toc-item-num\">0.0.7&nbsp;&nbsp;</span>Finally, the other (actually imporant) quantities in Bayes's rule are given by our model (i.e. we assumed X is distributed Gaussian and Y is Bernoulli)</a></span></li><li><span><a href=\"#So-let's-implement-this-:)\" data-toc-modified-id=\"So-let's-implement-this-:)-0.0.8\"><span class=\"toc-item-num\">0.0.8&nbsp;&nbsp;</span>So let's implement this :)</a></span></li><li><span><a href=\"#Maximum-likelihood-estimates-for-each-parameter:\" data-toc-modified-id=\"Maximum-likelihood-estimates-for-each-parameter:-0.0.9\"><span class=\"toc-item-num\">0.0.9&nbsp;&nbsp;</span>Maximum likelihood estimates for each parameter:</a></span></li><li><span><a href=\"#Heart-Disease-data-set-from-Kaggle:-https://www.kaggle.com/uciml/pima-indians-diabetes-database/data\" data-toc-modified-id=\"Heart-Disease-data-set-from-Kaggle:-https://www.kaggle.com/uciml/pima-indians-diabetes-database/data-0.0.10\"><span class=\"toc-item-num\">0.0.10&nbsp;&nbsp;</span>Heart Disease data set from Kaggle: <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database/data\" target=\"_blank\">https://www.kaggle.com/uciml/pima-indians-diabetes-database/data</a></a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model:\n",
    "\n",
    "$$ y \\sim Bernoulli(\\phi)$$\n",
    "$$ x \\mid y = 0 \\sim \\mathcal{N}(\\mu_0, \\Sigma)$$\n",
    "$$ x \\mid y = 1 \\sim \\mathcal{N}(\\mu_1, \\Sigma)$$\n",
    "<br>\n",
    "Notice that each Gaussian has the same covariance matrix $\\Sigma$. This is an additional simplifying homoscedasticity assumption of GDA (which is the same as LDA). This differs from Quadratic Discriminant Analysis which does not use this assumptions, implying that there is different covariance matrices for each Gaussian. See https://en.wikipedia.org/wiki/Linear_discriminant_analysis#LDA_for_two_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood of the data (b/c X is assumed iid):\n",
    "\n",
    "$$ \\mathcal{L}(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\log\\prod_{i=1}^m p(x^{(i)} \\mid y^{(i)}; \\mu_0, \\mu_1, \\Sigma)p(y^{(i)};\\phi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So maximizing this gives us the maximum likelihood estimates for our parameters. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:\n",
    "\n",
    "$$ p(y \\mid x) = \\frac{p(x \\mid y)p(y)}{p(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since are classification strategy will involve choosing the class that is most probable, we do not need to calculate the denominator of Bayes's rule because it is independent of y\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\underset{y}{\\mathrm{argmax}}p(y \\mid x) &= \\underset{y}{\\mathrm{argmax}} \\frac{p(x \\mid y) p(y)}{p(x)} \\\\\n",
    " &= \\underset{y}{\\mathrm{argmax}}p(x \\mid y)p(y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### However it is good to know that the denominator is given by:\n",
    "\n",
    "$$ p(x) = \\sum_{k=1}^{C} p(x \\mid y = k)p(y = k) $$\n",
    ", where $C$ is the number of classes. The quantities in the summation are given by our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the other (actually imporant) quantities in Bayes's rule are given by our model (i.e. we assumed X is distributed Gaussian and Y is Bernoulli)\n",
    "\n",
    "$$ p(y) = \\phi^y(1 - \\phi)^{(1-y)} $$\n",
    "<br>\n",
    "$$ p(x \\mid y = k) = \\dfrac{1}{(2\\pi)^{n/2} \\mid \\Sigma \\mid^{1/2}} \\exp\\Big(-\\frac12 (x-\\mu_k)^T \\Sigma^{-1}(x -\n",
    "\\mu_k)\\Big)$$\n",
    "<br>\n",
    "Note that $k \\in \\{0,1\\} $ because $Y$ is a binary variable in our specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So let's implement this :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimates for each parameter:\n",
    "\n",
    "$$ \\phi = \\frac{1}{m} \\sum_{i=1}^m 1\\{y^{(i)} = 1\\} $$\n",
    "This is just the fraction of positive class examples (for example, the number of emails that are spam).\n",
    "$$ \\mu_0 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}} $$ \n",
    "<br>\n",
    "$$ \\mu_1 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}} $$ \n",
    "or more generally,\n",
    "$$ \\mu_k = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}} $$ \n",
    ", for non-binary classification problems. Essentially, each of these formulas are saying to sum each example $x^{(i)}$ (which may be a vector) with class $y^{(i)} = k$ and average it over all examples with $y^{(i)} = k$.\n",
    "$$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}$$ \n",
    "This is the covariance matrix for the multivariate gaussian, which is a generalization of the variance of a real-valued random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 28.66666667,   7.        , -31.16666667],\n",
       "       [  7.        ,   5.55555556,  -3.88888889],\n",
       "       [-31.16666667,  -3.88888889,  40.80555556]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sk, sklearn.model_selection\n",
    "import pandas as pd\n",
    "\n",
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "Y = diabetes['Outcome']\n",
    "X = diabetes.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, Y, \n",
    "                                    test_size=0.33, \n",
    "                                    random_state=42)\n",
    "\n",
    "def my_covariance_matrix(X):\n",
    "    '''\n",
    "        Computes the covariance of X\n",
    "        X: array-like, shape = (n_samples, n_features)\n",
    "        Returns:\n",
    "            Covariance matrix of X, shape = (n_features, n_features)\n",
    "    '''\n",
    "    n_samples = X.shape[0]\n",
    "    X = X.T\n",
    "    avg = np.average(X, axis=1)\n",
    "    X = X.astype(np.float64)\n",
    "    X -= avg[:, np.newaxis]\n",
    "    return np.dot(X, X.T) / np.float64(n_samples)\n",
    "\n",
    "X = np.array([[-1, -5, 3], \n",
    "              [-2, -1, 5], \n",
    "              [-3, -2, 4], \n",
    "              [13, 1, -13], \n",
    "              [2, 1, 3], \n",
    "              [3, 2, 5]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "my_covariance_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (332,332) (182,182) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0702b88479b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mbgda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryGDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mbgda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbgda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mbgda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_covariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-0702b88479b8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_covariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-0702b88479b8>\u001b[0m in \u001b[0;36mcompute_covariance\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m    117\u001b[0m                              (X_train[i] - self.mu[c]).T)\n\u001b[1;32m    118\u001b[0m         '''\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# return co / (num_examples - len(classes))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (332,332) (182,182) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn as sk, sklearn.model_selection\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BinaryGDA():\n",
    "    \n",
    "    def __init(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.values\n",
    "        elif isinstance(X_train, list):\n",
    "            X_train = np.array(X_train)\n",
    "        #else:\n",
    "        #    raise ValueError('X_train should be 2d array or DataFrame')\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.values\n",
    "        elif isinstance(y_train, list):\n",
    "            y_train = np.array(y_train)\n",
    "        #else:\n",
    "        #    raise ValueError('y_train should be 1d array')\n",
    "        self.phi = self.compute_phi(y_train)\n",
    "        self.mu = self.compute_mu(X_train, y_train)\n",
    "        self.covariance = self.compute_covariance(X_train, y_train)\n",
    "        \n",
    "    def predict_bayes(self, X_test):\n",
    "        # Compute argmax{c_k} p(x | c_k) * p(c_k) / [sum(p(x | c_k) * p(c_k)) over classes]\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def class_conditional_p(self, c_k, x):\n",
    "        '''\n",
    "            Computes the class conditional probability p(x | c_k)\n",
    "            i.e. the mulitvariate gaussian equation with shared covariance\n",
    "            matrix and mu_k\n",
    "            c_k: the class to compute the probability from\n",
    "            x: the test x to compute from\n",
    "        '''\n",
    "        # the dimension of the gaussians\n",
    "        d = len(self.covariance)\n",
    "        pi_constant = (2 * np.pi) ** (1.0 / d)\n",
    "        covariance_constant = \\\n",
    "            np.linalg.det(self.covariance) ** (0.5)\n",
    "        constant =  1.0 / pi_constant * covariance_constant\n",
    "        \n",
    "        left = -0.5 * (x - self.mu[c_k]).T\n",
    "        middle = np.linalg.inv(self.covariance)\n",
    "        right = (x - self.mu[c_k])\n",
    "        exponent = np.exp(left * middle * right)\n",
    "        \n",
    "            \n",
    "    \n",
    "    def print_params(self):\n",
    "        #if self.phi == None or self.covariance  == None or self.mu == None:\n",
    "            #raise ValueError('You must fit the model first')\n",
    "        print('phi', self.phi)\n",
    "        print('mu', self.mu)\n",
    "        print('covariance matrix', self.covariance)\n",
    "    \n",
    "    def compute_mu(self, X_train, y_train):\n",
    "        '''\n",
    "        Computes the average feature vector for every class\n",
    "        X_train: \n",
    "            2d np array where rows are examples and columns are features,\n",
    "            all of which should be quantitative\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        classes = np.unique(y_train)\n",
    "        mu = []\n",
    "        for c in classes:\n",
    "            mu.append((y_train == c).dot(X_train) / np.sum(y_train == c))\n",
    "        return mu\n",
    "    \n",
    "    def compute_phi(self, y_train):\n",
    "        '''\n",
    "        Computes the proportion of \"success\" valued examples\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        num_examples = len(y_train)\n",
    "        classes = np.unique(y_train)\n",
    "        phi = []\n",
    "        for j in classes:\n",
    "            phi.append(np.sum(y_train == j) / num_examples)\n",
    "        return phi\n",
    "    \n",
    "    def compute_covariance(self, X_train, y_train):\n",
    "        '''\n",
    "        Computes the shared covariance matrix of the Gaussians fit to the data\n",
    "        i.e. it computes the pooled within-class covariance matrix used for each Gaussian\n",
    "        X_train: \n",
    "            2d np array where rows are examples and columns are features,\n",
    "            all of which should be quantitative\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        num_examples = len(y_train)\n",
    "        classes = np.unique(y_train)\n",
    "        # covariance = np.zeros((X_train.shape[1],X_train.shape[1]))\n",
    "        covs = []\n",
    "        for group in classes:\n",
    "            Xg = X_train[y_train == group, :]\n",
    "            # covs.append(np.atleast_2d(covariance.empirical_covariance(Xg)))\n",
    "            covs.append(np.atleast_2d(my_covariance_matrix(Xg)))\n",
    "        '''\n",
    "        for group in classes:\n",
    "            Xg = X[y == group, :]\n",
    "            covs.append(np.atleast_2d(_cov(Xg, shrinkage)))\n",
    "        return np.average(covs, axis=0, weights=priors)\n",
    "        for c in classes:\n",
    "            for i in range(num_examples):\n",
    "                if y_train[i] == c:\n",
    "                    covariance = np.outer(X_train[i] - self.mu[c], \n",
    "                             (X_train[i] - self.mu[c]).T)\n",
    "        '''\n",
    "        return np.average(covs, axis=0)\n",
    "        # return co / (num_examples - len(classes))\n",
    "\n",
    "bgda = BinaryGDA()\n",
    "bgda.fit(X_train, y_train)\n",
    "mu = bgda.compute_mu(X_train.values, y_train.values)\n",
    "bgda.compute_covariance(X_train.values, y_train.values)\n",
    "# c = bgda.compute_covariance(X_train.values, y_train.values)\n",
    "# np.linalg.inv(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.07190571e+01,  1.40955654e+01,  9.45769807e+00,\n",
       "        -3.14040334e+00, -2.01058684e+01,  8.61911611e-01,\n",
       "        -2.02742736e-02,  2.07322064e+01],\n",
       "       [ 1.40955654e+01,  1.04503706e+03,  1.00831451e+02,\n",
       "         1.11968009e+01,  1.30806211e+03,  4.90694235e+01,\n",
       "         1.97178482e+00,  1.01251011e+02],\n",
       "       [ 9.45769807e+00,  1.00831451e+02,  3.23684583e+02,\n",
       "         4.46035065e+01,  1.09750776e+02,  3.54084801e+01,\n",
       "         3.22380770e-01,  5.29893488e+01],\n",
       "       [-3.14040334e+00,  1.11968009e+01,  4.46035065e+01,\n",
       "         2.39829127e+02,  7.40715787e+02,  4.51210934e+01,\n",
       "         9.68970499e-01, -2.72674681e+01],\n",
       "       [-2.01058684e+01,  1.30806211e+03,  1.09750776e+02,\n",
       "         7.40715787e+02,  1.33471810e+04,  1.31763772e+02,\n",
       "         6.60843012e+00, -3.26425305e+01],\n",
       "       [ 8.61911611e-01,  4.90694235e+01,  3.54084801e+01,\n",
       "         4.51210934e+01,  1.31763772e+02,  5.83963085e+01,\n",
       "         3.86740053e-01,  7.44076368e-01],\n",
       "       [-2.02742736e-02,  1.97178482e+00,  3.22380770e-01,\n",
       "         9.68970499e-01,  6.60843012e+00,  3.86740053e-01,\n",
       "         1.16729167e-01,  1.70498501e-01],\n",
       "       [ 2.07322064e+01,  1.01251011e+02,  5.29893488e+01,\n",
       "        -2.72674681e+01, -3.26425305e+01,  7.44076368e-01,\n",
       "         1.70498501e-01,  1.37348862e+02]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import covariance\n",
    "covariance.empirical_covariance(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.15123191e+00,  2.00875667e+00,  4.03062241e+00,\n",
       "        -1.87662120e+00, -1.52936465e+01, -3.67253194e-01,\n",
       "        -2.67511154e-02,  9.29966025e+00],\n",
       "       [ 2.00875667e+00,  4.00616490e+02,  3.35245147e+01,\n",
       "        -1.81437453e+00,  5.27249899e+02,  5.22476848e+00,\n",
       "         5.83973884e-01,  2.48265571e+01],\n",
       "       [ 4.03062241e+00,  3.35245147e+01,  1.59501781e+02,\n",
       "         2.12746109e+01,  3.73081089e+01,  1.50285808e+01,\n",
       "         1.05499025e-01,  2.29198740e+01],\n",
       "       [-1.87662120e+00, -1.81437453e+00,  2.12746109e+01,\n",
       "         1.19463798e+02,  3.62648423e+02,  2.13863230e+01,\n",
       "         4.60044875e-01, -1.52025494e+01],\n",
       "       [-1.52936465e+01,  5.27249899e+02,  3.73081089e+01,\n",
       "         3.62648423e+02,  6.54173496e+03,  4.57990652e+01,\n",
       "         2.88620980e+00, -4.31528128e+01],\n",
       "       [-3.67253194e-01,  5.22476848e+00,  1.50285808e+01,\n",
       "         2.13863230e+01,  4.57990652e+01,  2.61393547e+01,\n",
       "         1.29703957e-01, -3.71465486e+00],\n",
       "       [-2.67511154e-02,  5.83973884e-01,  1.05499025e-01,\n",
       "         4.60044875e-01,  2.88620980e+00,  1.29703957e-01,\n",
       "         5.70394335e-02,  1.88532264e-04],\n",
       "       [ 9.29966025e+00,  2.48265571e+01,  2.29198740e+01,\n",
       "        -1.52025494e+01, -4.31528128e+01, -3.71465486e+00,\n",
       "         1.88532264e-04,  6.32144263e+01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgda.compute_covariance(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Disease data set from Kaggle: https://www.kaggle.com/uciml/pima-indians-diabetes-database/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.66773725e-01,  4.78383599e-04, -9.83036189e-04,\n",
       "        -2.16791633e-03,  4.04047531e-04, -9.14193784e-04,\n",
       "         1.21284721e-01, -3.93769517e-02],\n",
       "       [ 4.78383599e-04,  3.03636310e-03, -4.92542145e-04,\n",
       "         1.12373469e-03, -2.94901604e-04, -7.59369907e-04,\n",
       "        -2.23621457e-02, -1.05990087e-03],\n",
       "       [-9.83036189e-04, -4.92542145e-04,  7.28489227e-03,\n",
       "        -1.26120999e-03,  7.01888525e-05, -3.61826852e-03,\n",
       "         5.96508489e-03, -2.77128654e-03],\n",
       "       [-2.16791633e-03,  1.12373469e-03, -1.26120999e-03,\n",
       "         1.26522185e-02, -6.91528038e-04, -8.03136403e-03,\n",
       "        -5.89876457e-02,  2.43379838e-03],\n",
       "       [ 4.04047531e-04, -2.94901604e-04,  7.01888525e-05,\n",
       "        -6.91528038e-04,  2.15285601e-04,  2.30114720e-04,\n",
       "        -2.76051666e-03,  2.51156970e-05],\n",
       "       [-9.14193784e-04, -7.59369907e-04, -3.61826852e-03,\n",
       "        -8.03136403e-03,  2.30114720e-04,  4.72334354e-02,\n",
       "        -4.02446556e-02,  2.74590607e-03],\n",
       "       [ 1.21284721e-01, -2.23621457e-02,  5.96508489e-03,\n",
       "        -5.89876457e-02, -2.76051666e-03, -4.02446556e-02,\n",
       "         1.85135778e+01, -2.97135130e-02],\n",
       "       [-3.93769517e-02, -1.05990087e-03, -2.77128654e-03,\n",
       "         2.43379838e-03,  2.51156970e-05,  2.74590607e-03,\n",
       "        -2.97135130e-02,  2.37969881e-02]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgda = BinaryGDA()\n",
    "bgda.fit(X_train, y_train)\n",
    "c = bgda.compute_covariance(X_train.values, y_train.values)\n",
    "np.linalg.inv(c)\n",
    "#bgda.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BinaryGDA' object has no attribute 'covariance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-95985ab7e847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbgda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_conditional_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2d74dfa4c72e>\u001b[0m in \u001b[0;36mclass_conditional_p\u001b[0;34m(self, c_k, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         '''\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# the dimension of the gaussians\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mpi_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mcovariance_constant\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BinaryGDA' object has no attribute 'covariance'"
     ]
    }
   ],
   "source": [
    "bgda.class_conditional_p(1, X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(diabetes.SkinThickness)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
