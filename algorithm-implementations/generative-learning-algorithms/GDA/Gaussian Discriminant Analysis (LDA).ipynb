{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#The-model:\" data-toc-modified-id=\"The-model:-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>The model:</a></span></li><li><span><a href=\"#Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):\" data-toc-modified-id=\"Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):-0.0.2\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>Log-likelihood of the data (b/c X is assumed iid):</a></span></li><li><span><a href=\"#So-maximizing-this-gives-us-the-maximum-likelihood-estimates-for-our-parameters.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" data-toc-modified-id=\"So-maximizing-this-gives-us-the-maximum-likelihood-estimates-for-our-parameters.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf-0.0.3\"><span class=\"toc-item-num\">0.0.3&nbsp;&nbsp;</span>So maximizing this gives us the maximum likelihood estimates for our parameters. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: <a href=\"http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" target=\"_blank\">http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf</a></a></span></li><li><span><a href=\"#Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:\" data-toc-modified-id=\"Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:-0.0.4\"><span class=\"toc-item-num\">0.0.4&nbsp;&nbsp;</span>Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:</a></span></li><li><span><a href=\"#Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probable,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y\" data-toc-modified-id=\"Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probable,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y-0.0.5\"><span class=\"toc-item-num\">0.0.5&nbsp;&nbsp;</span>Since are classification strategy will involve choosing the class that is most probable, we do not need to calculate the denominator of Bayes's rule because it is independent of y</a></span></li><li><span><a href=\"#However-it-is-good-to-know-that-the-denominator-is-given-by:\" data-toc-modified-id=\"However-it-is-good-to-know-that-the-denominator-is-given-by:-0.0.6\"><span class=\"toc-item-num\">0.0.6&nbsp;&nbsp;</span>However it is good to know that the denominator is given by:</a></span></li><li><span><a href=\"#Finally,-the-other-(actually-imporant)-quantities-in-Bayes's-rule-are-given-by-our-model-(i.e.-we-assumed-X-is-distributed-Gaussian-and-Y-is-Bernoulli)\" data-toc-modified-id=\"Finally,-the-other-(actually-imporant)-quantities-in-Bayes's-rule-are-given-by-our-model-(i.e.-we-assumed-X-is-distributed-Gaussian-and-Y-is-Bernoulli)-0.0.7\"><span class=\"toc-item-num\">0.0.7&nbsp;&nbsp;</span>Finally, the other (actually imporant) quantities in Bayes's rule are given by our model (i.e. we assumed X is distributed Gaussian and Y is Bernoulli)</a></span></li><li><span><a href=\"#So-let's-implement-this-:)\" data-toc-modified-id=\"So-let's-implement-this-:)-0.0.8\"><span class=\"toc-item-num\">0.0.8&nbsp;&nbsp;</span>So let's implement this :)</a></span></li><li><span><a href=\"#Maximum-likelihood-estimates-for-each-parameter:\" data-toc-modified-id=\"Maximum-likelihood-estimates-for-each-parameter:-0.0.9\"><span class=\"toc-item-num\">0.0.9&nbsp;&nbsp;</span>Maximum likelihood estimates for each parameter:</a></span></li><li><span><a href=\"#Heart-Disease-data-set-from-Kaggle:-https://www.kaggle.com/uciml/pima-indians-diabetes-database/data\" data-toc-modified-id=\"Heart-Disease-data-set-from-Kaggle:-https://www.kaggle.com/uciml/pima-indians-diabetes-database/data-0.0.10\"><span class=\"toc-item-num\">0.0.10&nbsp;&nbsp;</span>Heart Disease data set from Kaggle: <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database/data\" target=\"_blank\">https://www.kaggle.com/uciml/pima-indians-diabetes-database/data</a></a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model:\n",
    "\n",
    "$$ y \\sim Bernoulli(\\phi)$$\n",
    "$$ x \\mid y = 0 \\sim \\mathcal{N}(\\mu_0, \\Sigma)$$\n",
    "$$ x \\mid y = 1 \\sim \\mathcal{N}(\\mu_1, \\Sigma)$$\n",
    "<br>\n",
    "Notice that each Gaussian has the same covariance matrix $\\Sigma$. This is an additional simplifying homoscedasticity assumption of GDA (which is the same as LDA). This differs from Quadratic Discriminant Analysis which does not use this assumptions, implying that there is different covariance matrices for each Gaussian. See https://en.wikipedia.org/wiki/Linear_discriminant_analysis#LDA_for_two_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood of the data (b/c X is assumed iid):\n",
    "\n",
    "$$ \\mathcal{L}(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\log\\prod_{i=1}^m p(x^{(i)} \\mid y^{(i)}; \\mu_0, \\mu_1, \\Sigma)p(y^{(i)};\\phi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So maximizing this gives us the maximum likelihood estimates for our parameters. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:\n",
    "\n",
    "$$ p(y \\mid x) = \\frac{p(x \\mid y)p(y)}{p(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since are classification strategy will involve choosing the class that is most probable, we do not need to calculate the denominator of Bayes's rule because it is independent of y\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\underset{y}{\\mathrm{argmax}}p(y \\mid x) &= \\underset{y}{\\mathrm{argmax}} \\frac{p(x \\mid y) p(y)}{p(x)} \\\\\n",
    " &= \\underset{y}{\\mathrm{argmax}}p(x \\mid y)p(y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### However it is good to know that the denominator is given by:\n",
    "\n",
    "$$ p(x) = \\sum_{k=1}^{C} p(x \\mid y = k)p(y = k) $$\n",
    ", where $C$ is the number of classes. The quantities in the summation are given by our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the other (actually imporant) quantities in Bayes's rule are given by our model (i.e. we assumed X is distributed Gaussian and Y is Bernoulli)\n",
    "\n",
    "$$ p(y) = \\phi^y(1 - \\phi)^{(1-y)} $$\n",
    "<br>\n",
    "$$ p(x \\mid y = k) = \\dfrac{1}{(2\\pi)^{n/2} \\mid \\Sigma \\mid^{1/2}} \\exp\\Big(-\\frac12 (x-\\mu_k)^T \\Sigma^{-1}(x -\n",
    "\\mu_k)\\Big)$$\n",
    "<br>\n",
    "Note that $k \\in \\{0,1\\} $ because $Y$ is a binary variable in our specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So let's implement this :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimates for each parameter:\n",
    "\n",
    "$$ \\phi = \\frac{1}{m} \\sum_{i=1}^m 1\\{y^{(i)} = 1\\} $$\n",
    "This is just the fraction of positive class examples (for example, the number of emails that are spam).\n",
    "$$ \\mu_0 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}} $$ \n",
    "<br>\n",
    "$$ \\mu_1 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}} $$ \n",
    "or more generally,\n",
    "$$ \\mu_k = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}} $$ \n",
    ", for non-binary classification problems. Essentially, each of these formulas are saying to sum each example $x^{(i)}$ (which may be a vector) with class $y^{(i)} = k$ and average it over all examples with $y^{(i)} = k$.\n",
    "$$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}$$ \n",
    "This is the covariance matrix for the multivariate gaussian, which is a generalization of the variance of a real-valued random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk, sklearn.model_selection\n",
    "\n",
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "Y = diabetes['Outcome']\n",
    "X = diabetes.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, Y, \n",
    "                                    test_size=0.33, \n",
    "                                    random_state=42)\n",
    "\n",
    "def temp(X):\n",
    "    X = X.T\n",
    "    avg = np.average(X, axis=1)\n",
    "    X -= avg[:, np.newaxis]\n",
    "    return np.dot(X, X.T) / len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.15123191e+00,  2.00875667e+00,  4.03062241e+00,\n",
       "        -1.87662120e+00, -1.52936465e+01, -3.67253194e-01,\n",
       "        -2.67511154e-02,  9.29966025e+00],\n",
       "       [ 2.00875667e+00,  4.00616490e+02,  3.35245147e+01,\n",
       "        -1.81437453e+00,  5.27249899e+02,  5.22476848e+00,\n",
       "         5.83973884e-01,  2.48265571e+01],\n",
       "       [ 4.03062241e+00,  3.35245147e+01,  1.59501781e+02,\n",
       "         2.12746109e+01,  3.73081089e+01,  1.50285808e+01,\n",
       "         1.05499025e-01,  2.29198740e+01],\n",
       "       [-1.87662120e+00, -1.81437453e+00,  2.12746109e+01,\n",
       "         1.19463798e+02,  3.62648423e+02,  2.13863230e+01,\n",
       "         4.60044875e-01, -1.52025494e+01],\n",
       "       [-1.52936465e+01,  5.27249899e+02,  3.73081089e+01,\n",
       "         3.62648423e+02,  6.54173496e+03,  4.57990652e+01,\n",
       "         2.88620980e+00, -4.31528128e+01],\n",
       "       [-3.67253194e-01,  5.22476848e+00,  1.50285808e+01,\n",
       "         2.13863230e+01,  4.57990652e+01,  2.61393547e+01,\n",
       "         1.29703957e-01, -3.71465486e+00],\n",
       "       [-2.67511154e-02,  5.83973884e-01,  1.05499025e-01,\n",
       "         4.60044875e-01,  2.88620980e+00,  1.29703957e-01,\n",
       "         5.70394335e-02,  1.88532264e-04],\n",
       "       [ 9.29966025e+00,  2.48265571e+01,  2.29198740e+01,\n",
       "        -1.52025494e+01, -4.31528128e+01, -3.71465486e+00,\n",
       "         1.88532264e-04,  6.32144263e+01]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn as sk, sklearn.model_selection\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BinaryGDA():\n",
    "    \n",
    "    def __init(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.values\n",
    "        elif isinstance(X_train, list):\n",
    "            X_train = np.array(X_train)\n",
    "        #else:\n",
    "        #    raise ValueError('X_train should be 2d array or DataFrame')\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.values\n",
    "        elif isinstance(y_train, list):\n",
    "            y_train = np.array(y_train)\n",
    "        #else:\n",
    "        #    raise ValueError('y_train should be 1d array')\n",
    "        self.phi = self.compute_phi(y_train)\n",
    "        self.mu = self.compute_mu(X_train, y_train)\n",
    "        # self.covariance = self.compute_covariance(X_train, y_train)\n",
    "        \n",
    "    def predict_bayes(self, X_test):\n",
    "        # Compute argmax{c_k} p(x | c_k) * p(c_k) / [sum(p(x | c_k) * p(c_k)) over classes]\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def class_conditional_p(self, c_k, x):\n",
    "        '''\n",
    "            Computes the class conditional probability p(x | c_k)\n",
    "            i.e. the mulitvariate gaussian equation with shared covariance\n",
    "            matrix and mu_k\n",
    "            c_k: the class to compute the probability from\n",
    "            x: the test x to compute from\n",
    "        '''\n",
    "        # the dimension of the gaussians\n",
    "        d = len(self.covariance)\n",
    "        pi_constant = (2 * np.pi) ** (1.0 / d)\n",
    "        covariance_constant = \\\n",
    "            np.linalg.det(self.covariance) ** (0.5)\n",
    "        constant =  1.0 / pi_constant * covariance_constant\n",
    "        \n",
    "        left = -0.5 * (x - self.mu[c_k]).T\n",
    "        middle = np.linalg.inv(self.covariance)\n",
    "        right = (x - self.mu[c_k])\n",
    "        exponent = np.exp(left * middle * right)\n",
    "        \n",
    "            \n",
    "    \n",
    "    def print_params(self):\n",
    "        #if self.phi == None or self.covariance  == None or self.mu == None:\n",
    "            #raise ValueError('You must fit the model first')\n",
    "        print('phi', self.phi)\n",
    "        print('mu', self.mu)\n",
    "        print('covariance matrix', self.covariance)\n",
    "    \n",
    "    def compute_mu(self, X_train, y_train):\n",
    "        '''\n",
    "        Computes the average feature vector for every class\n",
    "        X_train: \n",
    "            2d np array where rows are examples and columns are features,\n",
    "            all of which should be quantitative\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        classes = np.unique(y_train)\n",
    "        mu = []\n",
    "        for c in classes:\n",
    "            mu.append((y_train == c).dot(X_train) / np.sum(y_train == c))\n",
    "        return mu\n",
    "    \n",
    "    def compute_phi(self, y_train):\n",
    "        '''\n",
    "        Computes the proportion of \"success\" valued examples\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        num_examples = len(y_train)\n",
    "        classes = np.unique(y_train)\n",
    "        phi = []\n",
    "        for j in classes:\n",
    "            phi.append(np.sum(y_train == j) / num_examples)\n",
    "        return phi\n",
    "    \n",
    "    def compute_covariance(self, X_train, y_train):\n",
    "        '''\n",
    "        Computes the shared covariance matrix of the Gaussians fit to the data\n",
    "        i.e. it computes the pooled within-class covariance matrix used for each Gaussian\n",
    "        X_train: \n",
    "            2d np array where rows are examples and columns are features,\n",
    "            all of which should be quantitative\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        num_examples = len(y_train)\n",
    "        classes = np.unique(y_train)\n",
    "        # covariance = np.zeros((X_train.shape[1],X_train.shape[1]))\n",
    "        covs = []\n",
    "        for group in classes:\n",
    "            Xg = X_train[y_train == group, :]\n",
    "            # covs.append(np.atleast_2d(covariance.empirical_covariance(Xg)))\n",
    "            covs.append(np.atleast_2d(temp(Xg)))\n",
    "        '''\n",
    "        for group in classes:\n",
    "            Xg = X[y == group, :]\n",
    "            covs.append(np.atleast_2d(_cov(Xg, shrinkage)))\n",
    "        return np.average(covs, axis=0, weights=priors)\n",
    "        for c in classes:\n",
    "            for i in range(num_examples):\n",
    "                if y_train[i] == c:\n",
    "                    covariance = np.outer(X_train[i] - self.mu[c], \n",
    "                             (X_train[i] - self.mu[c]).T)\n",
    "        '''\n",
    "        return np.average(covs, axis=0)\n",
    "        # return co / (num_examples - len(classes))\n",
    "\n",
    "bgda = BinaryGDA()\n",
    "bgda.fit(X_train, y_train)\n",
    "mu = bgda.compute_mu(X_train.values, y_train.values)\n",
    "bgda.compute_covariance(X_train.values, y_train.values)\n",
    "# c = bgda.compute_covariance(X_train.values, y_train.values)\n",
    "# np.linalg.inv(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.32859572e-01, -1.04205728e-04, -3.93199882e-04,\n",
       "        -1.07775831e-03,  2.09972054e-04, -1.39733585e-03,\n",
       "         5.69012517e-02, -2.00531789e-02],\n",
       "       [-1.04205728e-04,  1.29454427e-03, -1.82241439e-04,\n",
       "         5.65904991e-04, -1.42274488e-04, -9.92014599e-04,\n",
       "        -1.36174661e-02, -7.67462902e-04],\n",
       "       [-3.93199882e-04, -1.82241439e-04,  3.62411380e-03,\n",
       "        -6.31761016e-04,  3.36123946e-05, -1.63381804e-03,\n",
       "         3.68010666e-03, -1.31764103e-03],\n",
       "       [-1.07775831e-03,  5.65904991e-04, -6.31761016e-04,\n",
       "         6.32603634e-03, -3.45857474e-04, -4.00462675e-03,\n",
       "        -2.94498352e-02,  1.22118734e-03],\n",
       "       [ 2.09972054e-04, -1.42274488e-04,  3.36123946e-05,\n",
       "        -3.45857474e-04,  1.07522990e-04,  1.29230359e-04,\n",
       "        -1.32386551e-03,  1.80553185e-05],\n",
       "       [-1.39733585e-03, -9.92014599e-04, -1.63381804e-03,\n",
       "        -4.00462675e-03,  1.29230359e-04,  2.19401297e-02,\n",
       "        -2.67932897e-02,  7.22632472e-04],\n",
       "       [ 5.69012517e-02, -1.36174661e-02,  3.68010666e-03,\n",
       "        -2.94498352e-02, -1.32386551e-03, -2.67932897e-02,\n",
       "         9.23024585e+00, -1.74443115e-02],\n",
       "       [-2.00531789e-02, -7.67462902e-04, -1.31764103e-03,\n",
       "         1.22118734e-03,  1.80553185e-05,  7.22632472e-04,\n",
       "        -1.74443115e-02,  1.16462455e-02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import covariance\n",
    "covariance.empirical_covariance(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.23013360e-02,  1.25855214e-02,  6.62687635e-01,\n",
       "         1.98221963e-01,  9.19592089e-01, -6.44408664e-02,\n",
       "         3.06881959e-03,  7.86595091e-02],\n",
       "       [ 1.25855214e-02,  3.74445266e-03,  1.97163263e-01,\n",
       "         5.89751294e-02,  2.73597646e-01, -1.91724892e-02,\n",
       "         9.13037233e-04,  2.34028291e-02],\n",
       "       [ 6.62687635e-01,  1.97163263e-01,  1.03815847e+01,\n",
       "         3.10532140e+00,  1.44062189e+01, -1.00952285e+00,\n",
       "         4.80757581e-02,  1.23227040e+00],\n",
       "       [ 1.98221963e-01,  5.89751294e-02,  3.10532140e+00,\n",
       "         9.28858289e-01,  4.30916293e+00, -3.01966704e-01,\n",
       "         1.43803364e-02,  3.68594559e-01],\n",
       "       [ 9.19592089e-01,  2.73597646e-01,  1.44062189e+01,\n",
       "         4.30916293e+00,  1.99910852e+01, -1.40088509e+00,\n",
       "         6.67133118e-02,  1.70998529e+00],\n",
       "       [-6.44408664e-02, -1.91724892e-02, -1.00952285e+00,\n",
       "        -3.01966704e-01, -1.40088509e+00,  9.81677094e-02,\n",
       "        -4.67496802e-03, -1.19828057e-01],\n",
       "       [ 3.06881959e-03,  9.13037233e-04,  4.80757581e-02,\n",
       "         1.43803364e-02,  6.67133118e-02, -4.67496802e-03,\n",
       "         2.22632535e-04,  5.70648271e-03],\n",
       "       [ 7.86595091e-02,  2.34028291e-02,  1.23227040e+00,\n",
       "         3.68594559e-01,  1.70998529e+00, -1.19828057e-01,\n",
       "         5.70648271e-03,  1.46267682e-01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgda.compute_covariance(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Disease data set from Kaggle: https://www.kaggle.com/uciml/pima-indians-diabetes-database/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1f698bdff26a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbgda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbgda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_covariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#bgda.print_params()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlenv/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "bgda = BinaryGDA()\n",
    "bgda.fit(X_train, y_train)\n",
    "c = bgda.compute_covariance(X_train.values, y_train.values)\n",
    "np.linalg.inv(c)\n",
    "#bgda.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgda.class_conditional_p(1, X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(diabetes.SkinThickness)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
