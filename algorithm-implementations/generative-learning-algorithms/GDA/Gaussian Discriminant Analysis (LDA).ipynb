{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Disease data set from Kaggle: https://www.kaggle.com/uciml/pima-indians-diabetes-database/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You will need to unzip pima-indians-diabetes-database.zip for this code to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#The-model:\" data-toc-modified-id=\"The-model:-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>The model:</a></span></li><li><span><a href=\"#Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):\" data-toc-modified-id=\"Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):-0.0.2\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>Log-likelihood of the data (b/c X is assumed iid):</a></span></li><li><span><a href=\"#So-maximizing-this-gives-us-the-maximum-likelihood-estimates-for-our-parameters.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" data-toc-modified-id=\"So-maximizing-this-gives-us-the-maximum-likelihood-estimates-for-our-parameters.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf-0.0.3\"><span class=\"toc-item-num\">0.0.3&nbsp;&nbsp;</span>So maximizing this gives us the maximum likelihood estimates for our parameters. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: <a href=\"http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" target=\"_blank\">http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf</a></a></span></li><li><span><a href=\"#Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:\" data-toc-modified-id=\"Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:-0.0.4\"><span class=\"toc-item-num\">0.0.4&nbsp;&nbsp;</span>Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:</a></span></li><li><span><a href=\"#Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probable,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y\" data-toc-modified-id=\"Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probable,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y-0.0.5\"><span class=\"toc-item-num\">0.0.5&nbsp;&nbsp;</span>Since are classification strategy will involve choosing the class that is most probable, we do not need to calculate the denominator of Bayes's rule because it is independent of y</a></span></li><li><span><a href=\"#However-it-is-good-to-know-that-the-denominator-is-given-by:\" data-toc-modified-id=\"However-it-is-good-to-know-that-the-denominator-is-given-by:-0.0.6\"><span class=\"toc-item-num\">0.0.6&nbsp;&nbsp;</span>However it is good to know that the denominator is given by:</a></span></li><li><span><a href=\"#Finally,-the-other-(actually-imporant)-quantities-in-Bayes's-rule-are-given-by-our-model-(i.e.-we-assumed-X-is-distributed-Gaussian-and-Y-is-Bernoulli)\" data-toc-modified-id=\"Finally,-the-other-(actually-imporant)-quantities-in-Bayes's-rule-are-given-by-our-model-(i.e.-we-assumed-X-is-distributed-Gaussian-and-Y-is-Bernoulli)-0.0.7\"><span class=\"toc-item-num\">0.0.7&nbsp;&nbsp;</span>Finally, the other (actually imporant) quantities in Bayes's rule are given by our model (i.e. we assumed X is distributed Gaussian and Y is Bernoulli)</a></span></li><li><span><a href=\"#So-let's-implement-this-:)\" data-toc-modified-id=\"So-let's-implement-this-:)-0.0.8\"><span class=\"toc-item-num\">0.0.8&nbsp;&nbsp;</span>So let's implement this :)</a></span></li><li><span><a href=\"#Maximum-likelihood-estimates-for-each-parameter:\" data-toc-modified-id=\"Maximum-likelihood-estimates-for-each-parameter:-0.0.9\"><span class=\"toc-item-num\">0.0.9&nbsp;&nbsp;</span>Maximum likelihood estimates for each parameter:</a></span></li><li><span><a href=\"#Heart-Disease-data-set-from-Kaggle:-https://www.kaggle.com/uciml/pima-indians-diabetes-database/data\" data-toc-modified-id=\"Heart-Disease-data-set-from-Kaggle:-https://www.kaggle.com/uciml/pima-indians-diabetes-database/data-0.0.10\"><span class=\"toc-item-num\">0.0.10&nbsp;&nbsp;</span>Heart Disease data set from Kaggle: <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database/data\" target=\"_blank\">https://www.kaggle.com/uciml/pima-indians-diabetes-database/data</a></a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model:\n",
    "\n",
    "$$ y \\sim Bernoulli(\\phi)$$\n",
    "$$ x \\mid y = 0 \\sim \\mathcal{N}(\\mu_0, \\Sigma)$$\n",
    "$$ x \\mid y = 1 \\sim \\mathcal{N}(\\mu_1, \\Sigma)$$\n",
    "<br>\n",
    "Notice that each Gaussian has the same covariance matrix $\\Sigma$. This is an additional simplifying homoscedasticity assumption of GDA (which is the same as LDA). This differs from Quadratic Discriminant Analysis which does not use this assumptions, implying that there is different covariance matrices for each Gaussian. See https://en.wikipedia.org/wiki/Linear_discriminant_analysis#LDA_for_two_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood of the data (b/c X is assumed iid):\n",
    "\n",
    "$$ \\mathcal{L}(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\log\\prod_{i=1}^m p(x^{(i)} \\mid y^{(i)}; \\mu_0, \\mu_1, \\Sigma)p(y^{(i)};\\phi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So maximizing this gives us the maximum likelihood estimates for our parameters. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:\n",
    "\n",
    "$$ p(y \\mid x) = \\frac{p(x \\mid y)p(y)}{p(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since are classification strategy will involve choosing the class that is most probable, we do not need to calculate the denominator of Bayes's rule because it is independent of y\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\underset{y}{\\mathrm{argmax}}p(y \\mid x) &= \\underset{y}{\\mathrm{argmax}} \\frac{p(x \\mid y) p(y)}{p(x)} \\\\\n",
    " &= \\underset{y}{\\mathrm{argmax}}p(x \\mid y)p(y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### However it is good to know that the denominator is given by:\n",
    "\n",
    "$$ p(x) = \\sum_{k=1}^{C} p(x \\mid y = k)p(y = k) $$\n",
    ", where $C$ is the number of classes. The quantities in the summation are given by our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the other (actually imporant) quantities in Bayes's rule are given by our model (i.e. we assumed X is distributed Gaussian and Y is Bernoulli)\n",
    "\n",
    "$$ p(y) = \\phi^y(1 - \\phi)^{(1-y)} $$\n",
    "<br>\n",
    "$$ p(x \\mid y = k) = \\dfrac{1}{(2\\pi)^{n/2} \\mid \\Sigma \\mid^{1/2}} \\exp\\Big(-\\frac12 (x-\\mu_k)^T \\Sigma^{-1}(x -\n",
    "\\mu_k)\\Big)$$\n",
    "<br>\n",
    "Note that $k \\in \\{0,1\\} $ because $Y$ is a binary variable in our specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So let's implement this :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimates for each parameter:\n",
    "\n",
    "$$ \\phi = \\frac{1}{m} \\sum_{i=1}^m 1\\{y^{(i)} = 1\\} $$\n",
    "This is just the fraction of positive class examples (for example, the number of emails that are spam).\n",
    "$$ \\mu_0 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}} $$ \n",
    "<br>\n",
    "$$ \\mu_1 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}} $$ \n",
    "or more generally,\n",
    "$$ \\mu_k = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}} $$ \n",
    ", for non-binary classification problems. Essentially, each of these formulas are saying to sum each example $x^{(i)}$ (which may be a vector) with class $y^{(i)} = k$ and average it over all examples with $y^{(i)} = k$.\n",
    "$$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}$$ \n",
    "This is the covariance matrix for the multivariate gaussian, which is a generalization of the variance of a real-valued random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk, sklearn.model_selection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "Y = diabetes['Outcome']\n",
    "X = diabetes.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, Y, \n",
    "                                    test_size=0.33, \n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "code_folding": [
     2,
     46,
     68,
     75,
     122
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class BinaryGDA():\n",
    "    \n",
    "    def __init(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.values\n",
    "        elif isinstance(X_train, list):\n",
    "            X_train = np.array(X_train)\n",
    "        #else:\n",
    "        #    raise ValueError('X_train should be 2d array or DataFrame')\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.values\n",
    "        elif isinstance(y_train, list):\n",
    "            y_train = np.array(y_train)\n",
    "        #else:\n",
    "        #    raise ValueError('y_train should be 1d array')\n",
    "        self.phi = self.compute_phi(y_train)\n",
    "        self.mu = self.compute_mu(X_train, y_train)\n",
    "        self.covariance = self.compute_covariance(X_train, y_train)\n",
    "        self.classes = np.unique(y_train)\n",
    "        \n",
    "    def predict_bayes(self, X_test, y_train):\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test = X_test.values\n",
    "        X_test = np.atleast_2d(X_test)\n",
    "        \n",
    "        # Compute argmax{c_k} p(x | c_k) * p(c_k) / [sum(p(x | c_k) * p(c_k)) over classes]\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            max_c = 0\n",
    "            max_p = 0\n",
    "            for c in self.classes:\n",
    "                p = self.compute_bayes(c, x)\n",
    "                if max_p < p:\n",
    "                    max_p = p\n",
    "                    max_c = c\n",
    "            predictions.append(max_c)\n",
    "        return predictions\n",
    "            \n",
    "    def compute_bayes(self, c, x):\n",
    "        # No need to compute denominator b/c it is not dependent on the class\n",
    "        return self.class_conditional_p(c, x) * self.phi[c]\n",
    "    \n",
    "    def class_conditional_p(self, c_k, x):\n",
    "        '''\n",
    "            Computes the class conditional probability p(x | c_k)\n",
    "            i.e. the mulitvariate gaussian equation with shared covariance\n",
    "            matrix and mu_k\n",
    "            c_k: the class to compute the probability from\n",
    "            x: the test x to compute from\n",
    "        '''\n",
    "        # the dimension of the gaussians\n",
    "        d = len(self.covariance)\n",
    "        pi_constant = (2 * np.pi) ** (1/d)\n",
    "        covariance_constant = \\\n",
    "            np.linalg.det(self.covariance) ** (0.5)            \n",
    "        constant =  1.0 / pi_constant * covariance_constant\n",
    "        \n",
    "        left = -0.5 * (x - self.mu[c_k]).T\n",
    "        middle = np.linalg.inv(self.covariance)\n",
    "        right = (x - self.mu[c_k])        \n",
    "        exponent = np.exp(left.dot(middle).dot(right))\n",
    "        return constant * exponent\n",
    "        \n",
    "            \n",
    "    \n",
    "    def print_params(self):\n",
    "        #if self.phi == None or self.covariance  == None or self.mu == None:\n",
    "            #raise ValueError('You must fit the model first')\n",
    "        print('phi', self.phi)\n",
    "        print('mu', self.mu)\n",
    "        print('covariance matrix', self.covariance)\n",
    "    \n",
    "    def compute_mu(self, X_train, y_train):\n",
    "        '''\n",
    "        Computes the average feature vector for every class\n",
    "        X_train: \n",
    "            2d np array where rows are examples and columns are features,\n",
    "            all of which should be quantitative\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        classes = np.unique(y_train)\n",
    "        mu = {}\n",
    "        for c in classes:\n",
    "            mu[c] = (y_train == c).dot(X_train) / np.sum(y_train == c)\n",
    "        return mu\n",
    "    \n",
    "    def compute_phi(self, y_train):\n",
    "        '''\n",
    "        Computes the proportion of \"success\" valued examples\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        num_examples = len(y_train)\n",
    "        classes = np.unique(y_train)\n",
    "        phi = {}\n",
    "        for j in classes:\n",
    "            phi[j] = (np.sum(y_train == j) / num_examples)\n",
    "        return phi\n",
    "    \n",
    "    def compute_covariance(self, X_train, y_train):\n",
    "        '''\n",
    "        Computes the shared covariance matrix of the Gaussians fit to the data\n",
    "        i.e. it computes the pooled within-class covariance matrix used for each Gaussian\n",
    "        X_train: \n",
    "            2d np array where rows are examples and columns are features,\n",
    "            all of which should be quantitative\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        num_examples = len(y_train)\n",
    "        classes = np.unique(y_train)\n",
    "        covs = []\n",
    "        for group in classes:\n",
    "            Xg = X_train[y_train == group, :]\n",
    "            from sklearn import covariance\n",
    "            covs.append(np.atleast_2d(self._cov(Xg)))\n",
    "        return np.average(covs, axis=0, weights=list(self.phi.values()))\n",
    "    \n",
    "    def _cov(self, X):\n",
    "        '''\n",
    "            Computes the covariance matrix of X, used to compute \n",
    "            covariance of instances in a class\n",
    "            X: array-like, shape = (n_samples, n_features)\n",
    "            Returns:\n",
    "                Covariance matrix of X, shape = (n_features, n_features)\n",
    "        '''\n",
    "        n_samples = X.shape[0]\n",
    "        X = X.T\n",
    "        avg = np.average(X, axis=1)\n",
    "        X = X.astype(np.float64)\n",
    "        X -= avg[:, np.newaxis]\n",
    "        return np.dot(X, X.T) / np.float64(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=True, tol=0.0001)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "X = np.array([[-1, -1], \n",
    "              [-2, -1], \n",
    "              [-3, -2], \n",
    "              [1, 1], \n",
    "              [2, 1], \n",
    "              [3, 2],[3, 9],[3, 9],[3, 9]])\n",
    "y = np.array([0,0,0,1,1,1,2,2,2])\n",
    "X_test = [[-1, -1], \n",
    "              [-2, -1], \n",
    "              [-3, -2], \n",
    "              [1, 1], \n",
    "              [2, 1], \n",
    "              [3, 2], [5,3], [7,9], [1,0]]\n",
    "clf = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking that we got all the parameters correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means estimated correctly: True\n",
      "Priors estimated correctly: True\n",
      "Covariance matrix estimated correctly: True\n"
     ]
    }
   ],
   "source": [
    "gda = BinaryGDA()\n",
    "gda.fit(X,y)\n",
    "print('Means estimated correctly:', np.array_equal(list(gda.mu.values()), clf.means_))\n",
    "print('Priors estimated correctly:', np.array_equal(list(gda.phi.values()), clf.priors_))\n",
    "print('Covariance matrix estimated correctly:', np.array_equal(gda.covariance, clf.covariance_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not sure how one prediction is different from the sklearn implementation when all the parameters are the same; it hints at something being wrong when I am calculating the class that gives maximum probability in predict_bayes function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gda = BinaryGDA()\n",
    "gda.fit(X,y)\n",
    "(np.equal(gda.predict_bayes(X_test, y), clf.predict(X_test)) == False).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
