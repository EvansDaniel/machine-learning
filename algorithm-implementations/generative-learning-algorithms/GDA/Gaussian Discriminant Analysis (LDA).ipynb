{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Disease data set from Kaggle: https://www.kaggle.com/uciml/pima-indians-diabetes-database/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You will need to unzip pima-indians-diabetes-database.zip for this code to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Heart-Disease-data-set-from-Kaggle:-https://www.kaggle.com/uciml/pima-indians-diabetes-database/data\" data-toc-modified-id=\"Heart-Disease-data-set-from-Kaggle:-https://www.kaggle.com/uciml/pima-indians-diabetes-database/data-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>Heart Disease data set from Kaggle: <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database/data\" target=\"_blank\">https://www.kaggle.com/uciml/pima-indians-diabetes-database/data</a></a></span></li></ul></li></ul></li><li><span><a href=\"#You-will-need-to-unzip-pima-indians-diabetes-database.zip-for-this-code-to-work\" data-toc-modified-id=\"You-will-need-to-unzip-pima-indians-diabetes-database.zip-for-this-code-to-work-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>You will need to unzip pima-indians-diabetes-database.zip for this code to work</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#The-model:\" data-toc-modified-id=\"The-model:-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>The model:</a></span></li><li><span><a href=\"#Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):\" data-toc-modified-id=\"Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Log-likelihood of the data (b/c X is assumed iid):</a></span></li><li><span><a href=\"#So-maximizing-this-w/-respect-to-each-parameter-gives-us-the-maximum-likelihood-estimates-for-that-parameter.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" data-toc-modified-id=\"So-maximizing-this-w/-respect-to-each-parameter-gives-us-the-maximum-likelihood-estimates-for-that-parameter.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>So maximizing this w/ respect to each parameter gives us the maximum likelihood estimates for that parameter. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: <a href=\"http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" target=\"_blank\">http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf</a></a></span></li><li><span><a href=\"#Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:\" data-toc-modified-id=\"Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:-1.0.4\"><span class=\"toc-item-num\">1.0.4&nbsp;&nbsp;</span>Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:</a></span></li><li><span><a href=\"#Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probable,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y\" data-toc-modified-id=\"Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probable,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y-1.0.5\"><span class=\"toc-item-num\">1.0.5&nbsp;&nbsp;</span>Since are classification strategy will involve choosing the class that is most probable, we do not need to calculate the denominator of Bayes's rule because it is independent of y</a></span></li><li><span><a href=\"#However-it-is-good-to-know-that-the-denominator-is-given-by:\" data-toc-modified-id=\"However-it-is-good-to-know-that-the-denominator-is-given-by:-1.0.6\"><span class=\"toc-item-num\">1.0.6&nbsp;&nbsp;</span>However it is good to know that the denominator is given by:</a></span></li><li><span><a href=\"#Finally,-the-other-(actually-imporant)-quantities-in-Bayes's-rule-are-given-by-our-model-(i.e.-we-assumed-X-is-distributed-Gaussian-and-Y-is-Bernoulli)\" data-toc-modified-id=\"Finally,-the-other-(actually-imporant)-quantities-in-Bayes's-rule-are-given-by-our-model-(i.e.-we-assumed-X-is-distributed-Gaussian-and-Y-is-Bernoulli)-1.0.7\"><span class=\"toc-item-num\">1.0.7&nbsp;&nbsp;</span>Finally, the other (actually imporant) quantities in Bayes's rule are given by our model (i.e. we assumed X is distributed Gaussian and Y is Bernoulli)</a></span></li><li><span><a href=\"#So-let's-implement-this-:)\" data-toc-modified-id=\"So-let's-implement-this-:)-1.0.8\"><span class=\"toc-item-num\">1.0.8&nbsp;&nbsp;</span>So let's implement this :)</a></span></li><li><span><a href=\"#Maximum-likelihood-estimates-for-each-parameter:\" data-toc-modified-id=\"Maximum-likelihood-estimates-for-each-parameter:-1.0.9\"><span class=\"toc-item-num\">1.0.9&nbsp;&nbsp;</span>Maximum likelihood estimates for each parameter:</a></span></li><li><span><a href=\"#Checking-that-we-got-all-the-parameters-correct\" data-toc-modified-id=\"Checking-that-we-got-all-the-parameters-correct-1.0.10\"><span class=\"toc-item-num\">1.0.10&nbsp;&nbsp;</span>Checking that we got all the parameters correct</a></span></li><li><span><a href=\"#Not-sure-how-one-prediction-is-different-from-the-sklearn-implementation-when-all-the-parameters-are-the-same;-it-hints-at-something-being-wrong-when-I-am-calculating-the-class-that-gives-maximum-probability-in-predict_bayes-function\" data-toc-modified-id=\"Not-sure-how-one-prediction-is-different-from-the-sklearn-implementation-when-all-the-parameters-are-the-same;-it-hints-at-something-being-wrong-when-I-am-calculating-the-class-that-gives-maximum-probability-in-predict_bayes-function-1.0.11\"><span class=\"toc-item-num\">1.0.11&nbsp;&nbsp;</span>Not sure how one prediction is different from the sklearn implementation when all the parameters are the same; it hints at something being wrong when I am calculating the class that gives maximum probability in predict_bayes function</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model:\n",
    "\n",
    "$$ y \\sim Bernoulli(\\phi)$$\n",
    "$$ x \\mid y = 0 \\sim \\mathcal{N}(\\mu_0, \\Sigma)$$\n",
    "$$ x \\mid y = 1 \\sim \\mathcal{N}(\\mu_1, \\Sigma)$$\n",
    "<br>\n",
    "Notice that each Gaussian has the same covariance matrix $\\Sigma$. This is an additional simplifying homoscedasticity assumption of GDA (which is the same as LDA). This differs from Quadratic Discriminant Analysis which does not use this assumptions, implying that there is different covariance matrices for each Gaussian. See https://en.wikipedia.org/wiki/Linear_discriminant_analysis#LDA_for_two_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood of the data (b/c X is assumed iid):\n",
    "\n",
    "$$ \\mathcal{L}(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\log\\prod_{i=1}^m p(x^{(i)} \\mid y^{(i)}; \\mu_0, \\mu_1, \\Sigma)p(y^{(i)};\\phi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So maximizing this w/ respect to each parameter gives us the maximum likelihood estimates for that parameter. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:\n",
    "\n",
    "$$ p(y \\mid x) = \\frac{p(x \\mid y)p(y)}{p(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since are classification strategy will involve choosing the class that is most probable, we do not need to calculate the denominator of Bayes's rule because it is independent of y\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\underset{y}{\\mathrm{argmax}}p(y \\mid x) &= \\underset{y}{\\mathrm{argmax}} \\frac{p(x \\mid y) p(y)}{p(x)} \\\\\n",
    " &= \\underset{y}{\\mathrm{argmax}}p(x \\mid y)p(y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### However it is good to know that the denominator is given by:\n",
    "\n",
    "$$ p(x) = \\sum_{k=1}^{C} p(x \\mid y = k)p(y = k) $$\n",
    ", where $C$ is the number of classes. The quantities in the summation are given by our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the other (actually imporant) quantities in Bayes's rule are given by our model (i.e. we assumed X is distributed Gaussian and Y is Bernoulli)\n",
    "\n",
    "$$ p(y) = \\phi^y(1 - \\phi)^{(1-y)} $$\n",
    "<br>\n",
    "$$ p(x \\mid y = k) = \\dfrac{1}{(2\\pi)^{n/2} \\mid \\Sigma \\mid^{1/2}} \\exp\\Big(-\\frac12 (x-\\mu_k)^T \\Sigma^{-1}(x -\n",
    "\\mu_k)\\Big)$$\n",
    "<br>\n",
    "Note that $k \\in \\{0,1\\} $ because $Y$ is a binary variable in our specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So let's implement this :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimates for each parameter:\n",
    "\n",
    "$$ \\phi = \\frac{1}{m} \\sum_{i=1}^m 1\\{y^{(i)} = 1\\} $$\n",
    "This is just the fraction of positive class examples (for example, the number of emails that are spam).\n",
    "$$ \\mu_0 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}} $$ \n",
    "<br>\n",
    "$$ \\mu_1 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}} $$ \n",
    "or more generally,\n",
    "$$ \\mu_k = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}} $$ \n",
    ", for non-binary classification problems. Essentially, each of these formulas are saying to sum each example $x^{(i)}$ (which may be a vector) with class $y^{(i)} = k$ and average it over all examples with $y^{(i)} = k$.\n",
    "$$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}$$ \n",
    "This is the covariance matrix for the multivariate gaussian, which is a generalization of the variance of a real-valued random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk, sklearn.model_selection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "Y = diabetes['Outcome']\n",
    "X = diabetes[['BloodPressure', 'Glucose']]#diabetes.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, Y, \n",
    "                                    test_size=0.33, \n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "code_folding": [
     2,
     34,
     41,
     70,
     98,
     131,
     138,
     153,
     166,
     185
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class GDA():\n",
    "    \n",
    "    def __init(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "            Fits a Gaussian to each class in y_train using the training data\n",
    "            X_train: array-like, shape = (n_examples, n_features)\n",
    "            y_train: array-like, shape = (n_examples, ), training labels \n",
    "        '''\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.values\n",
    "        elif isinstance(X_train, list):\n",
    "            X_train = np.array(X_train)\n",
    "        #else:\n",
    "        #    raise ValueError('X_train should be 2d array or DataFrame')\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.values\n",
    "        elif isinstance(y_train, list):\n",
    "            y_train = np.array(y_train)\n",
    "        #else:\n",
    "        #    raise ValueError('y_train should be 1d array')\n",
    "        self.phi = self.compute_phi(y_train)\n",
    "        self.mu = self.compute_mu(X_train, y_train)\n",
    "        self.covariance = self.compute_covariance(X_train, y_train)\n",
    "        self.classes = np.unique(y_train)\n",
    "        return self\n",
    "        \n",
    "    def predict_bayes(self, X_test):\n",
    "        '''\n",
    "            Predicts the class that each example of X_test belongs to \n",
    "            X_test: array-like, shape = (n_examples, n_features)\n",
    "        '''\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test = X_test.values\n",
    "        X_test = np.atleast_2d(X_test)\n",
    "        \n",
    "        # Compute argmax{c_k} p(x | c_k) * p(c_k) / [sum(p(x | c_k) * p(c_k)) over classes]\n",
    "        '''\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            # Keep track of max class and max (unormalized) probability\n",
    "            max_c = 0\n",
    "            max_p = 0\n",
    "            for c in self.classes:\n",
    "                p = self.compute_bayes(c, x)\n",
    "                #print(p, c)\n",
    "                if max_p < p:\n",
    "                    max_p = p\n",
    "                    max_c = c\n",
    "            predictions.append(max_c)\n",
    "        return np.array(predictions)\n",
    "        '''\n",
    "        predictions = []\n",
    "        for c in self.classes:\n",
    "            Xp = self.compute_bayes(c, X_test)\n",
    "            predictions.append(Xp)\n",
    "        \n",
    "        max_class_indices = np.argmax(np.array(predictions).T, axis=1)\n",
    "        # map indices back to the classes \n",
    "        return np.array([self.classes[i] for i in max_class_indices])\n",
    "        \n",
    "            \n",
    "    def compute_bayes(self, c, x):\n",
    "        '''\n",
    "            Computes the numerator of Bayes's rule\n",
    "            c: the class to compute the conditional probability density with\n",
    "            x: the instance to classify, array-like, shape = (n_features, )\n",
    "        '''\n",
    "        # No need to compute denominator b/c it is not dependent on the class\n",
    "        return self.class_conditional_p(c, x) * self.phi[c]\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        '''\n",
    "            Computes the probability of each example of the in X_test\n",
    "            of belonging to each class \n",
    "            X_test: array-like, shape = (n_examples, n_features)\n",
    "        '''\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test = X_test.values\n",
    "        X_test = np.atleast_2d(X_test)\n",
    "        \n",
    "        # Compute argmax{c_k} p(x | c_k) * p(c_k) / [sum(p(x | c_k) * p(c_k)) over classes]\n",
    "        '''\n",
    "        predictions = []\n",
    "        class_probs = []\n",
    "        for x in X_test:\n",
    "            norm = self.compute_normalization(x)\n",
    "            for c in self.classes:\n",
    "                # Normalize by denominator to get probabilities\n",
    "                p = self.compute_bayes(c, x) / norm\n",
    "                class_probs.append(p)\n",
    "            predictions.append(class_probs)\n",
    "        return np.array(predictions)\n",
    "        '''\n",
    "        for c in self.classes:\n",
    "            Xp = self.compute_bayes(c, X_test) / norm\n",
    "    \n",
    "    # \n",
    "    def compute_normalization(self, X):\n",
    "        '''\n",
    "            Computes the normalization factor in denominator of Bayes's rule\n",
    "            x: array-like, shape = (n_features, )\n",
    "        '''\n",
    "        return np.sum([self.compute_bayes(c, X) for c in self.classes], axis=0)\n",
    "    \n",
    "    def class_conditional_p(self, c_k, X):\n",
    "        '''\n",
    "            Computes the class conditional probability p(x | c_k)\n",
    "            i.e. the mulitvariate gaussian equation with shared covariance\n",
    "            matrix and mu_k\n",
    "            c_k: the class to compute the probability from\n",
    "            x: the test x to compute from\n",
    "        '''\n",
    "        # the dimension of the gaussians\n",
    "        d = len(self.covariance)\n",
    "        pi_constant = (2 * np.pi) ** (1/d)\n",
    "        covariance_constant = \\\n",
    "            np.linalg.det(self.covariance) ** (0.5)            \n",
    "        constant =  1.0 / pi_constant * covariance_constant\n",
    "        \n",
    "        left = -0.5 * (X - self.mu[c_k]).T\n",
    "        middle = np.linalg.inv(self.covariance)\n",
    "        right = (X - self.mu[c_k])   \n",
    "        # print(left[:, 0].shape, middle.shape, right[0, :].shape)\n",
    "        preds = []\n",
    "        for i in range(len(X)):\n",
    "            preds.append(left[:, i].dot(middle).dot(right[i, :]))\n",
    "        exponent = np.exp(preds)\n",
    "        # should be shape = (n_examples,)\n",
    "        return constant * exponent\n",
    "    \n",
    "    def print_params(self):\n",
    "        #if self.phi == None or self.covariance  == None or self.mu == None:\n",
    "            #raise ValueError('You must fit the model first')\n",
    "        print('phi', self.phi)\n",
    "        print('mu', self.mu)\n",
    "        print('covariance matrix', self.covariance)\n",
    "    \n",
    "    def compute_mu(self, X_train, y_train):\n",
    "        '''\n",
    "        Computes the average feature vector for every class\n",
    "        X_train: \n",
    "            2d np array where rows are examples and columns are features,\n",
    "            all of which should be quantitative\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        classes = np.unique(y_train)\n",
    "        mu = {}\n",
    "        for c in classes:\n",
    "            mu[c] = (y_train == c).dot(X_train) / np.sum(y_train == c)\n",
    "        return mu\n",
    "    \n",
    "    def compute_phi(self, y_train):\n",
    "        '''\n",
    "        Computes the proportion of \"success\" valued examples\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        num_examples = len(y_train)\n",
    "        classes = np.unique(y_train)\n",
    "        phi = {}\n",
    "        for j in classes:\n",
    "            phi[j] = (np.sum(y_train == j) / num_examples)\n",
    "        return phi\n",
    "    \n",
    "    def compute_covariance(self, X_train, y_train):\n",
    "        '''\n",
    "        Computes the shared covariance matrix of the Gaussians fit to the data\n",
    "        i.e. it computes the pooled within-class covariance matrix used for each Gaussian\n",
    "        X_train: \n",
    "            2d np array where rows are examples and columns are features,\n",
    "            all of which should be quantitative\n",
    "        y_train: \n",
    "            1d np array representing labels in [0, 1]\n",
    "        '''\n",
    "        num_examples = len(y_train)\n",
    "        classes = np.unique(y_train)\n",
    "        covs = []\n",
    "        for group in classes:\n",
    "            Xg = X_train[y_train == group, :]\n",
    "            from sklearn import covariance\n",
    "            covs.append(np.atleast_2d(self._cov(Xg)))\n",
    "        return np.average(covs, axis=0, weights=list(self.phi.values()))\n",
    "    \n",
    "    def _cov(self, X):\n",
    "        '''\n",
    "            Computes the covariance matrix of X, used to compute \n",
    "            covariance of instances in a class\n",
    "            X: array-like, shape = (n_samples, n_features)\n",
    "            Returns:\n",
    "                Covariance matrix of X, shape = (n_features, n_features)\n",
    "        '''\n",
    "        n_samples = X.shape[0]\n",
    "        X = X.T\n",
    "        avg = np.average(X, axis=1)\n",
    "        X = X.astype(np.float64)\n",
    "        X -= avg[:, np.newaxis]\n",
    "        return np.dot(X, X.T) / np.float64(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7362204724409449"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "clf.fit(X_train, y_train)\n",
    "clf_predictions = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, clf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking that we got all the parameters correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means estimated correctly: True\n",
      "Priors estimated correctly: True\n",
      "Covariance matrix estimated correctly: False\n"
     ]
    }
   ],
   "source": [
    "gda = GDA()\n",
    "gda.fit(X_train,y_train)\n",
    "print('Means estimated correctly:', np.array_equal(list(gda.mu.values()), clf.means_))\n",
    "print('Priors estimated correctly:', np.array_equal(list(gda.phi.values()), clf.priors_))\n",
    "print('Covariance matrix estimated correctly:', np.array_equal(gda.covariance, clf.covariance_))\n",
    "predictions = gda.predict_bayes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not sure how one prediction is different from the sklearn implementation when all the parameters are the same; it hints at something being wrong when I am calculating the class that gives maximum probability in predict_bayes function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7362204724409449"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     11,
     21,
     32,
     110,
     127
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAACECAYAAAA6AA+3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6860a6e0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import linalg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# #############################################################################\n",
    "# Colormap\n",
    "cmap = colors.LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate datasets\n",
    "def dataset_fixed_cov():\n",
    "    '''Generate 2 Gaussians samples with the same covariance matrix'''\n",
    "    n, dim = 300, 2\n",
    "    np.random.seed(0)\n",
    "    C = np.array([[0., -0.23], [0.83, .23]])\n",
    "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
    "              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def dataset_cov():\n",
    "    '''Generate 2 Gaussians samples with different covariance matrices'''\n",
    "    n, dim = 300, 2\n",
    "    np.random.seed(0)\n",
    "    C = np.array([[0., -1.], [2.5, .7]]) * 2.\n",
    "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
    "              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot functions\n",
    "def plot_data(lda, X, y, y_pred, fig_index):\n",
    "    splot = plt.subplot(2, 2, fig_index)\n",
    "    if fig_index == 1:\n",
    "        plt.title('Linear Discriminant Analysis')\n",
    "        plt.ylabel('Data with\\n fixed covariance')\n",
    "    elif fig_index == 2:\n",
    "        plt.title('Quadratic Discriminant Analysis')\n",
    "    elif fig_index == 3:\n",
    "        plt.ylabel('Data with\\n varying covariances')\n",
    "\n",
    "    # Gives array w/ True = correct classification\n",
    "    tp = (y == y_pred)  # True Positive\n",
    "    tp0, tp1 = tp[y == 0], tp[y == 1]\n",
    "    X0, X1 = X[y == 0], X[y == 1]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "\n",
    "    alpha = 0.5\n",
    "\n",
    "    # class 0: dots\n",
    "    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='red', markeredgecolor='k')\n",
    "    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#990000', markeredgecolor='k')  # dark red\n",
    "\n",
    "    # class 1: dots\n",
    "    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='blue', markeredgecolor='k')\n",
    "    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#000099', markeredgecolor='k')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    # We want to make predictions at every point in the plane \n",
    "    # so as to color the decision regions\n",
    "    nx, ny = 200, 100\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    # a grid of each point in the plane (obvs not technically, there are infinite # of points)\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                         np.linspace(y_min, y_max, ny))\n",
    "    # make a prediction for each point in the plane\n",
    "    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    # xx, yy represent the points to plot the color at\n",
    "    # Z is mapped onto the color palette, norm causes color mapping\n",
    "    # in the range specified \n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=colors.Normalize(0., 1.))\n",
    "    \n",
    "    # Plot a contour of Z at xx, yy at the .5 level\n",
    "    # See: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contour.html\n",
    "    CS = plt.contour(xx, yy, Z, [.5], linewidths=1., colors='k')\n",
    "    plt.clabel(CS, fontsize=10)\n",
    "\n",
    "    # means: the mean of each feature value for each Gaussian\n",
    "    # first gaussian: mean of 0th and 1th feature\n",
    "    plt.plot(lda.means_[0][0], lda.means_[0][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "    # second gaussian: mean of 0th and 1th feature\n",
    "    plt.plot(lda.means_[1][0], lda.means_[1][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "\n",
    "    return splot\n",
    "\n",
    "# Plots the covariance ellipses \n",
    "def plot_ellipse(splot, mean, cov, color):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    u = w[0] / linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n",
    "                              180 + angle, facecolor=color,\n",
    "                              edgecolor='yellow',\n",
    "                              linewidth=2, zorder=2)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(0.5)\n",
    "    splot.add_artist(ell)\n",
    "    splot.set_xticks(())\n",
    "    splot.set_yticks(())\n",
    "\n",
    "\n",
    "def plot_lda_cov(lda, splot):\n",
    "    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n",
    "    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n",
    "    \n",
    "# Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "y_pred = lda.fit(X_train, y_train).predict(X_train)\n",
    "splot = plot_data(lda, X_train.values, y_train.values, y_pred, fig_index=1)\n",
    "plot_lda_cov(lda, splot)\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     8,
     20,
     23,
     25,
     84,
     101
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here0.5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,20000) and (2,2) not aligned: 20000 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d0d8552ffb78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0msplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;31m# plot_lda_cov(lda, splot)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d0d8552ffb78>\u001b[0m in \u001b[0;36mplot_data\u001b[0;34m(lda, X, y, y_pred, fig_index)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'here0.5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# make a prediction for each point in the plane\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'here1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-61db9f8760f1>\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X_test)\u001b[0m\n\u001b[1;32m     85\u001b[0m         '''\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mXp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-61db9f8760f1>\u001b[0m in \u001b[0;36mcompute_bayes\u001b[0;34m(self, c, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m         '''\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# No need to compute denominator b/c it is not dependent on the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_conditional_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-61db9f8760f1>\u001b[0m in \u001b[0;36mclass_conditional_p\u001b[0;34m(self, c_k, X)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mmiddle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mexponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiddle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mexponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,20000) and (2,2) not aligned: 20000 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "from scipy import linalg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "\n",
    "# #############################################################################\n",
    "# Colormap\n",
    "cmap = colors.LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot functions\n",
    "def plot_data(lda, X, y, y_pred, fig_index):\n",
    "    splot = plt.subplot(2, 2, fig_index)\n",
    "    if fig_index == 1:\n",
    "        plt.title('Linear Discriminant Analysis')\n",
    "        plt.ylabel('Data with\\n fixed covariance')\n",
    "    elif fig_index == 2:\n",
    "        plt.title('Quadratic Discriminant Analysis')\n",
    "    elif fig_index == 3:\n",
    "        plt.ylabel('Data with\\n varying covariances')\n",
    "\n",
    "    # Gives array w/ True = correct classification\n",
    "    tp = (y == y_pred)  # True Positive\n",
    "    tp0, tp1 = tp[y == 0], tp[y == 1]\n",
    "    X0, X1 = X[y == 0], X[y == 1]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "\n",
    "    alpha = 0.5\n",
    "\n",
    "    # class 0: dots\n",
    "    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='red', markeredgecolor='k')\n",
    "    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#990000', markeredgecolor='k')  # dark red\n",
    "\n",
    "    # class 1: dots\n",
    "    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='blue', markeredgecolor='k')\n",
    "    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#000099', markeredgecolor='k')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    # We want to make predictions at every point in the plane \n",
    "    # so as to color the decision regions\n",
    "    nx, ny = 200, 100\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    print('here')\n",
    "    # a grid of each point in the plane (obvs not technically, there are infinite # of points)\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                         np.linspace(y_min, y_max, ny))\n",
    "    print('here0.5')\n",
    "    # make a prediction for each point in the plane\n",
    "    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    print('here1')\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    # xx, yy represent the points to plot the color at\n",
    "    # Z is mapped onto the color palette, norm causes color mapping\n",
    "    # in the range specified \n",
    "    print('here2')\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=colors.Normalize(0., 1.))\n",
    "    print('here3')\n",
    "    \n",
    "    # Plot a contour of Z at xx, yy at the .5 level\n",
    "    # See: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contour.html\n",
    "    CS = plt.contour(xx, yy, Z, [.5], linewidths=1., colors='k')\n",
    "    print('here4')\n",
    "    plt.clabel(CS, fontsize=10)\n",
    "\n",
    "    # means: the mean of each feature value for each Gaussian\n",
    "    # first gaussian: mean of 0th and 1th feature\n",
    "    plt.plot(lda.mu[0][0], lda.mu[0][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "    print('here5')\n",
    "    # second gaussian: mean of 0th and 1th feature\n",
    "    plt.plot(lda.mu[1][0], lda.mu[1][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "    print('here6')\n",
    "\n",
    "    return splot\n",
    "\n",
    "# Plots the covariance ellipses \n",
    "def plot_ellipse(splot, mean, cov, color):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    u = w[0] / linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n",
    "                              180 + angle, facecolor=color,\n",
    "                              edgecolor='yellow',\n",
    "                              linewidth=2, zorder=2)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(0.5)\n",
    "    splot.add_artist(ell)\n",
    "    splot.set_xticks(())\n",
    "    splot.set_yticks(())\n",
    "\n",
    "\n",
    "def plot_lda_cov(lda, splot):\n",
    "    plot_ellipse(splot, lda.mu[0], lda.covariance, 'red')\n",
    "    plot_ellipse(splot, lda.mu[1], lda.covariance, 'blue')\n",
    "    \n",
    "# Linear Discriminant Analysis\n",
    "lda = GDA()\n",
    "y_pred = lda.fit(X_train, y_train).predict_bayes(X_train)\n",
    "splot = plot_data(lda, X_train.values, y_train.values, y_pred, fig_index=1)\n",
    "# plot_lda_cov(lda, splot)\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
