{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#The-model:\" data-toc-modified-id=\"The-model:-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>The model:</a></span></li><li><span><a href=\"#Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):\" data-toc-modified-id=\"Log-likelihood-of-the-data-(b/c-X-is-assumed-iid):-0.0.2\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>Log-likelihood of the data (b/c X is assumed iid):</a></span></li><li><span><a href=\"#So-maximizing-this-gives-us-the-maximum-likelihood-estimates-for-our-parameters.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" data-toc-modified-id=\"So-maximizing-this-gives-us-the-maximum-likelihood-estimates-for-our-parameters.-We-can-do-so-by-(unsurprisingly)-taking-the-partial-derivative-w/-respect-to-each-parameter,-set-it-equal-to-0,-and-solve-the-equation.-Reference:-http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf-0.0.3\"><span class=\"toc-item-num\">0.0.3&nbsp;&nbsp;</span>So maximizing this gives us the maximum likelihood estimates for our parameters. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: <a href=\"http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf\" target=\"_blank\">http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf</a></a></span></li><li><span><a href=\"#Maximum-likelihood-estimates-for-each-parameter:\" data-toc-modified-id=\"Maximum-likelihood-estimates-for-each-parameter:-0.0.4\"><span class=\"toc-item-num\">0.0.4&nbsp;&nbsp;</span>Maximum likelihood estimates for each parameter:</a></span></li><li><span><a href=\"#Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:\" data-toc-modified-id=\"Using-these-parameter-estimates,-we-can-model-$p(y\\mid-x)$,-the-posterior-distribution,-using-Bayes's-rule:-0.0.5\"><span class=\"toc-item-num\">0.0.5&nbsp;&nbsp;</span>Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:</a></span></li><li><span><a href=\"#Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probably,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y\" data-toc-modified-id=\"Since-are-classification-strategy-will-involve-choosing-the-class-that-is-most-probably,-we-do-not-need-to-calculate-the-denominator-of-Bayes's-rule-because-it-is-independent-of-y-0.0.6\"><span class=\"toc-item-num\">0.0.6&nbsp;&nbsp;</span>Since are classification strategy will involve choosing the class that is most probably, we do not need to calculate the denominator of Bayes's rule because it is independent of y</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model:\n",
    "\n",
    "$$ y \\sim Bernoulli(\\phi)$$\n",
    "$$ x \\mid y = 0 \\sim \\mathcal{N}(\\mu_0, \\Sigma)$$\n",
    "$$ x \\mid y = 1 \\sim \\mathcal{N}(\\mu_1, \\Sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood of the data (b/c X is assumed iid):\n",
    "\n",
    "$$ \\mathcal{L}(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\log\\prod_{i=1}^m p(x^{(i)} \\mid y^{(i)}; \\mu_0, \\mu_1, \\Sigma)p(y^{(i)};\\phi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So maximizing this gives us the maximum likelihood estimates for our parameters. We can do so by (unsurprisingly) taking the partial derivative w/ respect to each parameter, set it equal to 0, and solve the equation. Reference: http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimates for each parameter:\n",
    "\n",
    "$$ \\phi = \\frac{1}{m} \\sum_{i=1}^m 1\\{y^{(i)} = 1\\} $$\n",
    "This is just the fraction of positive class examples (for example, the number of emails that are spam).\n",
    "$$ \\mu_0 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}} $$ \n",
    "<br>\n",
    "$$ \\mu_1 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}} $$ \n",
    "or more generally,\n",
    "$$ \\mu_k = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = k\\}} $$ \n",
    ", for non-binary classification problems. Essentially, each of these formulas are saying to sum each example $x^{(i)}$ (which may be a vector) with class $y^{(i)} = k$ and average it over all examples with $y^{(i)} = k$.\n",
    "$$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}$$ \n",
    "This is the covariance matrix for the multivariate gaussian, which is a generalization of the variance of a real-valued random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using these parameter estimates, we can model $p(y\\mid x)$, the posterior distribution, using Bayes's rule:\n",
    "\n",
    "$$ p(y \\mid x) = \\frac{p(x \\mid y)p(y)}{p(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since are classification strategy will involve choosing the class that is most probably, we do not need to calculate the denominator of Bayes's rule because it is independent of y\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\underset{y}{\\mathrm{argmax}}p(y \\mid x) &= \\underset{y}{\\mathrm{argmax}} \\frac{p(x \\mid y) p(y)}{p(x)} \\\\\n",
    " &= \\underset{y}{\\mathrm{argmax}}p(x \\mid y)p(y)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "a = \\frac{1}{2} && b = \\frac{1}{3} && c = \\frac{1}{4} \\\\\n",
    "a && b && c\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "  q u^{q-1} u &= p x^{p-1} \\\\\n",
    "            u &= \\frac{p x^{-1}}{q u^{-1}} \\\\\n",
    "            u &= n x^{n-1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
